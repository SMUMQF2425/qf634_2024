{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a88a8ce7-88c3-4c86-9e9f-015f954e8c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSFT</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>DEXJPUS</th>\n",
       "      <th>DEXUSUK</th>\n",
       "      <th>DJIA</th>\n",
       "      <th>VIXCLS</th>\n",
       "      <th>AAPL_1DT</th>\n",
       "      <th>AAPL_3DT</th>\n",
       "      <th>AAPL_6DT</th>\n",
       "      <th>AAPL_12DT</th>\n",
       "      <th>...</th>\n",
       "      <th>sia_cleaned_Comment_tokens_joined_lemmatized_pos</th>\n",
       "      <th>sia_cleaned_Comment_tokens_joined_lemmatized_compound</th>\n",
       "      <th>sia_cleaned_title_self_tokens_joined_lemmatized_neg</th>\n",
       "      <th>sia_cleaned_title_self_tokens_joined_lemmatized_neu</th>\n",
       "      <th>sia_cleaned_title_self_tokens_joined_lemmatized_pos</th>\n",
       "      <th>sia_cleaned_title_self_tokens_joined_lemmatized_compound</th>\n",
       "      <th>negative_finbert_comment</th>\n",
       "      <th>neutral_finbert_comment</th>\n",
       "      <th>positive_finbert_comment</th>\n",
       "      <th>finbert_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-11</th>\n",
       "      <td>4.977415</td>\n",
       "      <td>4.205386</td>\n",
       "      <td>4.688316</td>\n",
       "      <td>0.275812</td>\n",
       "      <td>10.236787</td>\n",
       "      <td>2.707383</td>\n",
       "      <td>0.008493</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.042705</td>\n",
       "      <td>0.033765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144750</td>\n",
       "      <td>0.252900</td>\n",
       "      <td>0.035250</td>\n",
       "      <td>0.759500</td>\n",
       "      <td>0.205000</td>\n",
       "      <td>0.914275</td>\n",
       "      <td>0.300685</td>\n",
       "      <td>0.635741</td>\n",
       "      <td>0.063574</td>\n",
       "      <td>-0.300685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12</th>\n",
       "      <td>4.987516</td>\n",
       "      <td>4.208535</td>\n",
       "      <td>4.692815</td>\n",
       "      <td>0.272543</td>\n",
       "      <td>10.244665</td>\n",
       "      <td>2.634762</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.016866</td>\n",
       "      <td>0.036463</td>\n",
       "      <td>0.018928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133571</td>\n",
       "      <td>0.489700</td>\n",
       "      <td>0.028143</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.143143</td>\n",
       "      <td>0.484814</td>\n",
       "      <td>0.077541</td>\n",
       "      <td>0.784491</td>\n",
       "      <td>0.137968</td>\n",
       "      <td>-0.077541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-13</th>\n",
       "      <td>4.995899</td>\n",
       "      <td>4.207333</td>\n",
       "      <td>4.693913</td>\n",
       "      <td>0.288856</td>\n",
       "      <td>10.244783</td>\n",
       "      <td>2.536075</td>\n",
       "      <td>0.013501</td>\n",
       "      <td>0.024540</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>0.040269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100667</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.394833</td>\n",
       "      <td>0.049167</td>\n",
       "      <td>-0.120667</td>\n",
       "      <td>0.182847</td>\n",
       "      <td>0.756541</td>\n",
       "      <td>0.060613</td>\n",
       "      <td>-0.182847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-16</th>\n",
       "      <td>5.002349</td>\n",
       "      <td>4.217549</td>\n",
       "      <td>4.697202</td>\n",
       "      <td>0.287432</td>\n",
       "      <td>10.248349</td>\n",
       "      <td>2.496506</td>\n",
       "      <td>0.016973</td>\n",
       "      <td>0.033020</td>\n",
       "      <td>0.033241</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146500</td>\n",
       "      <td>0.246837</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.736125</td>\n",
       "      <td>0.210875</td>\n",
       "      <td>0.646775</td>\n",
       "      <td>0.264257</td>\n",
       "      <td>0.644742</td>\n",
       "      <td>0.091001</td>\n",
       "      <td>-0.264257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-17</th>\n",
       "      <td>4.996934</td>\n",
       "      <td>4.213270</td>\n",
       "      <td>4.696107</td>\n",
       "      <td>0.271248</td>\n",
       "      <td>10.249456</td>\n",
       "      <td>2.508786</td>\n",
       "      <td>0.001963</td>\n",
       "      <td>0.032438</td>\n",
       "      <td>0.049304</td>\n",
       "      <td>0.048068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103286</td>\n",
       "      <td>0.114757</td>\n",
       "      <td>0.094429</td>\n",
       "      <td>0.688857</td>\n",
       "      <td>0.217000</td>\n",
       "      <td>0.640243</td>\n",
       "      <td>0.119574</td>\n",
       "      <td>0.825515</td>\n",
       "      <td>0.054911</td>\n",
       "      <td>-0.119574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-26</th>\n",
       "      <td>5.450865</td>\n",
       "      <td>4.584228</td>\n",
       "      <td>4.972933</td>\n",
       "      <td>0.067939</td>\n",
       "      <td>10.284004</td>\n",
       "      <td>3.473828</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>-0.019377</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>-0.024180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140333</td>\n",
       "      <td>0.027250</td>\n",
       "      <td>0.070667</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.111333</td>\n",
       "      <td>0.437117</td>\n",
       "      <td>0.324316</td>\n",
       "      <td>0.595453</td>\n",
       "      <td>0.080231</td>\n",
       "      <td>-0.324316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-27</th>\n",
       "      <td>5.446476</td>\n",
       "      <td>4.577379</td>\n",
       "      <td>4.974732</td>\n",
       "      <td>0.072600</td>\n",
       "      <td>10.279695</td>\n",
       "      <td>3.484312</td>\n",
       "      <td>0.006545</td>\n",
       "      <td>-0.006437</td>\n",
       "      <td>-0.017764</td>\n",
       "      <td>-0.036299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237727</td>\n",
       "      <td>0.489700</td>\n",
       "      <td>0.082818</td>\n",
       "      <td>0.662273</td>\n",
       "      <td>0.164091</td>\n",
       "      <td>0.262518</td>\n",
       "      <td>0.122306</td>\n",
       "      <td>0.798727</td>\n",
       "      <td>0.078967</td>\n",
       "      <td>-0.122306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-28</th>\n",
       "      <td>5.465995</td>\n",
       "      <td>4.603197</td>\n",
       "      <td>4.970854</td>\n",
       "      <td>0.079920</td>\n",
       "      <td>10.298355</td>\n",
       "      <td>3.407179</td>\n",
       "      <td>-0.012732</td>\n",
       "      <td>-0.003930</td>\n",
       "      <td>-0.046041</td>\n",
       "      <td>-0.086817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176500</td>\n",
       "      <td>0.474720</td>\n",
       "      <td>0.107600</td>\n",
       "      <td>0.583700</td>\n",
       "      <td>0.108700</td>\n",
       "      <td>0.101580</td>\n",
       "      <td>0.339672</td>\n",
       "      <td>0.579781</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.339672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29</th>\n",
       "      <td>5.451075</td>\n",
       "      <td>4.576558</td>\n",
       "      <td>4.972933</td>\n",
       "      <td>0.099664</td>\n",
       "      <td>10.282801</td>\n",
       "      <td>3.460723</td>\n",
       "      <td>-0.050366</td>\n",
       "      <td>-0.056554</td>\n",
       "      <td>-0.075931</td>\n",
       "      <td>-0.076712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150600</td>\n",
       "      <td>0.423050</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.591000</td>\n",
       "      <td>0.121900</td>\n",
       "      <td>0.445760</td>\n",
       "      <td>0.277046</td>\n",
       "      <td>0.666928</td>\n",
       "      <td>0.056026</td>\n",
       "      <td>-0.277046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-30</th>\n",
       "      <td>5.431517</td>\n",
       "      <td>4.558223</td>\n",
       "      <td>4.974732</td>\n",
       "      <td>0.107418</td>\n",
       "      <td>10.265541</td>\n",
       "      <td>3.453790</td>\n",
       "      <td>-0.030500</td>\n",
       "      <td>-0.093598</td>\n",
       "      <td>-0.100035</td>\n",
       "      <td>-0.116721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120889</td>\n",
       "      <td>0.433178</td>\n",
       "      <td>0.067000</td>\n",
       "      <td>0.500556</td>\n",
       "      <td>0.099111</td>\n",
       "      <td>0.372389</td>\n",
       "      <td>0.422049</td>\n",
       "      <td>0.491158</td>\n",
       "      <td>0.086792</td>\n",
       "      <td>-0.422049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>688 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                MSFT     GOOGL   DEXJPUS   DEXUSUK       DJIA    VIXCLS  \\\n",
       "2019-12-11  4.977415  4.205386  4.688316  0.275812  10.236787  2.707383   \n",
       "2019-12-12  4.987516  4.208535  4.692815  0.272543  10.244665  2.634762   \n",
       "2019-12-13  4.995899  4.207333  4.693913  0.288856  10.244783  2.536075   \n",
       "2019-12-16  5.002349  4.217549  4.697202  0.287432  10.248349  2.496506   \n",
       "2019-12-17  4.996934  4.213270  4.696107  0.271248  10.249456  2.508786   \n",
       "...              ...       ...       ...       ...        ...       ...   \n",
       "2022-09-26  5.450865  4.584228  4.972933  0.067939  10.284004  3.473828   \n",
       "2022-09-27  5.446476  4.577379  4.974732  0.072600  10.279695  3.484312   \n",
       "2022-09-28  5.465995  4.603197  4.970854  0.079920  10.298355  3.407179   \n",
       "2022-09-29  5.451075  4.576558  4.972933  0.099664  10.282801  3.460723   \n",
       "2022-09-30  5.431517  4.558223  4.974732  0.107418  10.265541  3.453790   \n",
       "\n",
       "            AAPL_1DT  AAPL_3DT  AAPL_6DT  AAPL_12DT  ...  \\\n",
       "2019-12-11  0.008493  0.000221  0.042705   0.033765  ...   \n",
       "2019-12-12  0.002545  0.016866  0.036463   0.018928  ...   \n",
       "2019-12-13  0.013501  0.024540  0.035400   0.040269  ...   \n",
       "2019-12-16  0.016973  0.033020  0.033241   0.043900  ...   \n",
       "2019-12-17  0.001963  0.032438  0.049304   0.048068  ...   \n",
       "...              ...       ...       ...        ...  ...   \n",
       "2022-09-26  0.002258 -0.019377  0.000464  -0.024180  ...   \n",
       "2022-09-27  0.006545 -0.006437 -0.017764  -0.036299  ...   \n",
       "2022-09-28 -0.012732 -0.003930 -0.046041  -0.086817  ...   \n",
       "2022-09-29 -0.050366 -0.056554 -0.075931  -0.076712  ...   \n",
       "2022-09-30 -0.030500 -0.093598 -0.100035  -0.116721  ...   \n",
       "\n",
       "            sia_cleaned_Comment_tokens_joined_lemmatized_pos  \\\n",
       "2019-12-11                                          0.144750   \n",
       "2019-12-12                                          0.133571   \n",
       "2019-12-13                                          0.100667   \n",
       "2019-12-16                                          0.146500   \n",
       "2019-12-17                                          0.103286   \n",
       "...                                                      ...   \n",
       "2022-09-26                                          0.140333   \n",
       "2022-09-27                                          0.237727   \n",
       "2022-09-28                                          0.176500   \n",
       "2022-09-29                                          0.150600   \n",
       "2022-09-30                                          0.120889   \n",
       "\n",
       "            sia_cleaned_Comment_tokens_joined_lemmatized_compound  \\\n",
       "2019-12-11                                           0.252900       \n",
       "2019-12-12                                           0.489700       \n",
       "2019-12-13                                           0.169000       \n",
       "2019-12-16                                           0.246837       \n",
       "2019-12-17                                           0.114757       \n",
       "...                                                       ...       \n",
       "2022-09-26                                           0.027250       \n",
       "2022-09-27                                           0.489700       \n",
       "2022-09-28                                           0.474720       \n",
       "2022-09-29                                           0.423050       \n",
       "2022-09-30                                           0.433178       \n",
       "\n",
       "            sia_cleaned_title_self_tokens_joined_lemmatized_neg  \\\n",
       "2019-12-11                                           0.035250     \n",
       "2019-12-12                                           0.028143     \n",
       "2019-12-13                                           0.056000     \n",
       "2019-12-16                                           0.053000     \n",
       "2019-12-17                                           0.094429     \n",
       "...                                                       ...     \n",
       "2022-09-26                                           0.070667     \n",
       "2022-09-27                                           0.082818     \n",
       "2022-09-28                                           0.107600     \n",
       "2022-09-29                                           0.087200     \n",
       "2022-09-30                                           0.067000     \n",
       "\n",
       "            sia_cleaned_title_self_tokens_joined_lemmatized_neu  \\\n",
       "2019-12-11                                           0.759500     \n",
       "2019-12-12                                           0.685714     \n",
       "2019-12-13                                           0.394833     \n",
       "2019-12-16                                           0.736125     \n",
       "2019-12-17                                           0.688857     \n",
       "...                                                       ...     \n",
       "2022-09-26                                           0.818000     \n",
       "2022-09-27                                           0.662273     \n",
       "2022-09-28                                           0.583700     \n",
       "2022-09-29                                           0.591000     \n",
       "2022-09-30                                           0.500556     \n",
       "\n",
       "            sia_cleaned_title_self_tokens_joined_lemmatized_pos  \\\n",
       "2019-12-11                                           0.205000     \n",
       "2019-12-12                                           0.143143     \n",
       "2019-12-13                                           0.049167     \n",
       "2019-12-16                                           0.210875     \n",
       "2019-12-17                                           0.217000     \n",
       "...                                                       ...     \n",
       "2022-09-26                                           0.111333     \n",
       "2022-09-27                                           0.164091     \n",
       "2022-09-28                                           0.108700     \n",
       "2022-09-29                                           0.121900     \n",
       "2022-09-30                                           0.099111     \n",
       "\n",
       "            sia_cleaned_title_self_tokens_joined_lemmatized_compound  \\\n",
       "2019-12-11                                           0.914275          \n",
       "2019-12-12                                           0.484814          \n",
       "2019-12-13                                          -0.120667          \n",
       "2019-12-16                                           0.646775          \n",
       "2019-12-17                                           0.640243          \n",
       "...                                                       ...          \n",
       "2022-09-26                                           0.437117          \n",
       "2022-09-27                                           0.262518          \n",
       "2022-09-28                                           0.101580          \n",
       "2022-09-29                                           0.445760          \n",
       "2022-09-30                                           0.372389          \n",
       "\n",
       "            negative_finbert_comment  neutral_finbert_comment  \\\n",
       "2019-12-11                  0.300685                 0.635741   \n",
       "2019-12-12                  0.077541                 0.784491   \n",
       "2019-12-13                  0.182847                 0.756541   \n",
       "2019-12-16                  0.264257                 0.644742   \n",
       "2019-12-17                  0.119574                 0.825515   \n",
       "...                              ...                      ...   \n",
       "2022-09-26                  0.324316                 0.595453   \n",
       "2022-09-27                  0.122306                 0.798727   \n",
       "2022-09-28                  0.339672                 0.579781   \n",
       "2022-09-29                  0.277046                 0.666928   \n",
       "2022-09-30                  0.422049                 0.491158   \n",
       "\n",
       "            positive_finbert_comment  finbert_compound  \n",
       "2019-12-11                  0.063574         -0.300685  \n",
       "2019-12-12                  0.137968         -0.077541  \n",
       "2019-12-13                  0.060613         -0.182847  \n",
       "2019-12-16                  0.091001         -0.264257  \n",
       "2019-12-17                  0.054911         -0.119574  \n",
       "...                              ...               ...  \n",
       "2022-09-26                  0.080231         -0.324316  \n",
       "2022-09-27                  0.078967         -0.122306  \n",
       "2022-09-28                  0.080548         -0.339672  \n",
       "2022-09-29                  0.056026         -0.277046  \n",
       "2022-09-30                  0.086792         -0.422049  \n",
       "\n",
       "[688 rows x 32 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from QLSTM import SequenceDataset\n",
    "\n",
    "# import yfinance as yf\n",
    "from pandas_datareader import data as pdr\n",
    "# yf.pdr_override()\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import scienceplots\n",
    "\n",
    "from QLSTM import ShallowRegressionLSTM\n",
    "\n",
    "# Calculate the RMSE for the train and test data\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define hyperparameter grid\n",
    "batch_sizes = [5]\n",
    "sequence_lengths = [1, 2, 3, 4, 5]\n",
    "learning_rates = [0.01, 0.001]\n",
    "hidden_units = [16, 32]\n",
    "num_epochs_list = [20, 50]\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_grid = product(batch_sizes, sequence_lengths, learning_rates, hidden_units, num_epochs_list)\n",
    "\n",
    "# Load the data\n",
    "X = pd.read_csv(\"X.csv\", index_col=0)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3b9399e-dff3-4276-bc5d-b7092acad9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSFT', 'GOOGL', 'DEXJPUS', 'DEXUSUK', 'DJIA', 'VIXCLS', 'AAPL_1DT',\n",
       "       'AAPL_3DT', 'AAPL_6DT', 'AAPL_12DT', 'SMA_21', 'SMA_63', 'SMA_252',\n",
       "       'EMA_10', 'EMA_30', 'EMA_200', 'RSI_10', 'RSI_30', 'RSI_200', 'apple',\n",
       "       'sia_cleaned_Comment_tokens_joined_lemmatized_neg',\n",
       "       'sia_cleaned_Comment_tokens_joined_lemmatized_neu',\n",
       "       'sia_cleaned_Comment_tokens_joined_lemmatized_pos',\n",
       "       'sia_cleaned_Comment_tokens_joined_lemmatized_compound',\n",
       "       'sia_cleaned_title_self_tokens_joined_lemmatized_neg',\n",
       "       'sia_cleaned_title_self_tokens_joined_lemmatized_neu',\n",
       "       'sia_cleaned_title_self_tokens_joined_lemmatized_pos',\n",
       "       'sia_cleaned_title_self_tokens_joined_lemmatized_compound',\n",
       "       'negative_finbert_comment', 'neutral_finbert_comment',\n",
       "       'positive_finbert_comment', 'finbert_compound'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cdfd673-7827-42cc-944c-0771f97b3f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-11</th>\n",
       "      <td>4.183982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12</th>\n",
       "      <td>4.186527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-13</th>\n",
       "      <td>4.200029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-16</th>\n",
       "      <td>4.217002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-17</th>\n",
       "      <td>4.218966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-26</th>\n",
       "      <td>5.003637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-27</th>\n",
       "      <td>5.010182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-28</th>\n",
       "      <td>4.997449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29</th>\n",
       "      <td>4.947083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-30</th>\n",
       "      <td>4.916583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>688 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            AAPL_pred\n",
       "2019-12-11   4.183982\n",
       "2019-12-12   4.186527\n",
       "2019-12-13   4.200029\n",
       "2019-12-16   4.217002\n",
       "2019-12-17   4.218966\n",
       "...               ...\n",
       "2022-09-26   5.003637\n",
       "2022-09-27   5.010182\n",
       "2022-09-28   4.997449\n",
       "2022-09-29   4.947083\n",
       "2022-09-30   4.916583\n",
       "\n",
       "[688 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = pd.read_csv(\"Y.csv\", index_col=0)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc77e7b9-836c-4891-80f1-323996e3b4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSFT</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>DEXJPUS</th>\n",
       "      <th>DEXUSUK</th>\n",
       "      <th>DJIA</th>\n",
       "      <th>VIXCLS</th>\n",
       "      <th>AAPL_1DT</th>\n",
       "      <th>AAPL_3DT</th>\n",
       "      <th>AAPL_6DT</th>\n",
       "      <th>AAPL_12DT</th>\n",
       "      <th>...</th>\n",
       "      <th>sia_cleaned_Comment_tokens_joined_lemmatized_compound</th>\n",
       "      <th>sia_cleaned_title_self_tokens_joined_lemmatized_neg</th>\n",
       "      <th>sia_cleaned_title_self_tokens_joined_lemmatized_neu</th>\n",
       "      <th>sia_cleaned_title_self_tokens_joined_lemmatized_pos</th>\n",
       "      <th>sia_cleaned_title_self_tokens_joined_lemmatized_compound</th>\n",
       "      <th>negative_finbert_comment</th>\n",
       "      <th>neutral_finbert_comment</th>\n",
       "      <th>positive_finbert_comment</th>\n",
       "      <th>finbert_compound</th>\n",
       "      <th>AAPL_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-11</th>\n",
       "      <td>4.977415</td>\n",
       "      <td>4.205386</td>\n",
       "      <td>4.688316</td>\n",
       "      <td>0.275812</td>\n",
       "      <td>10.236787</td>\n",
       "      <td>2.707383</td>\n",
       "      <td>0.008493</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.042705</td>\n",
       "      <td>0.033765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252900</td>\n",
       "      <td>0.035250</td>\n",
       "      <td>0.759500</td>\n",
       "      <td>0.205000</td>\n",
       "      <td>0.914275</td>\n",
       "      <td>0.300685</td>\n",
       "      <td>0.635741</td>\n",
       "      <td>0.063574</td>\n",
       "      <td>-0.300685</td>\n",
       "      <td>4.183982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12</th>\n",
       "      <td>4.987516</td>\n",
       "      <td>4.208535</td>\n",
       "      <td>4.692815</td>\n",
       "      <td>0.272543</td>\n",
       "      <td>10.244665</td>\n",
       "      <td>2.634762</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.016866</td>\n",
       "      <td>0.036463</td>\n",
       "      <td>0.018928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.489700</td>\n",
       "      <td>0.028143</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.143143</td>\n",
       "      <td>0.484814</td>\n",
       "      <td>0.077541</td>\n",
       "      <td>0.784491</td>\n",
       "      <td>0.137968</td>\n",
       "      <td>-0.077541</td>\n",
       "      <td>4.186527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-13</th>\n",
       "      <td>4.995899</td>\n",
       "      <td>4.207333</td>\n",
       "      <td>4.693913</td>\n",
       "      <td>0.288856</td>\n",
       "      <td>10.244783</td>\n",
       "      <td>2.536075</td>\n",
       "      <td>0.013501</td>\n",
       "      <td>0.024540</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>0.040269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.394833</td>\n",
       "      <td>0.049167</td>\n",
       "      <td>-0.120667</td>\n",
       "      <td>0.182847</td>\n",
       "      <td>0.756541</td>\n",
       "      <td>0.060613</td>\n",
       "      <td>-0.182847</td>\n",
       "      <td>4.200029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-16</th>\n",
       "      <td>5.002349</td>\n",
       "      <td>4.217549</td>\n",
       "      <td>4.697202</td>\n",
       "      <td>0.287432</td>\n",
       "      <td>10.248349</td>\n",
       "      <td>2.496506</td>\n",
       "      <td>0.016973</td>\n",
       "      <td>0.033020</td>\n",
       "      <td>0.033241</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246837</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.736125</td>\n",
       "      <td>0.210875</td>\n",
       "      <td>0.646775</td>\n",
       "      <td>0.264257</td>\n",
       "      <td>0.644742</td>\n",
       "      <td>0.091001</td>\n",
       "      <td>-0.264257</td>\n",
       "      <td>4.217002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-17</th>\n",
       "      <td>4.996934</td>\n",
       "      <td>4.213270</td>\n",
       "      <td>4.696107</td>\n",
       "      <td>0.271248</td>\n",
       "      <td>10.249456</td>\n",
       "      <td>2.508786</td>\n",
       "      <td>0.001963</td>\n",
       "      <td>0.032438</td>\n",
       "      <td>0.049304</td>\n",
       "      <td>0.048068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114757</td>\n",
       "      <td>0.094429</td>\n",
       "      <td>0.688857</td>\n",
       "      <td>0.217000</td>\n",
       "      <td>0.640243</td>\n",
       "      <td>0.119574</td>\n",
       "      <td>0.825515</td>\n",
       "      <td>0.054911</td>\n",
       "      <td>-0.119574</td>\n",
       "      <td>4.218966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-26</th>\n",
       "      <td>5.450865</td>\n",
       "      <td>4.584228</td>\n",
       "      <td>4.972933</td>\n",
       "      <td>0.067939</td>\n",
       "      <td>10.284004</td>\n",
       "      <td>3.473828</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>-0.019377</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>-0.024180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027250</td>\n",
       "      <td>0.070667</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.111333</td>\n",
       "      <td>0.437117</td>\n",
       "      <td>0.324316</td>\n",
       "      <td>0.595453</td>\n",
       "      <td>0.080231</td>\n",
       "      <td>-0.324316</td>\n",
       "      <td>5.003637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-27</th>\n",
       "      <td>5.446476</td>\n",
       "      <td>4.577379</td>\n",
       "      <td>4.974732</td>\n",
       "      <td>0.072600</td>\n",
       "      <td>10.279695</td>\n",
       "      <td>3.484312</td>\n",
       "      <td>0.006545</td>\n",
       "      <td>-0.006437</td>\n",
       "      <td>-0.017764</td>\n",
       "      <td>-0.036299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.489700</td>\n",
       "      <td>0.082818</td>\n",
       "      <td>0.662273</td>\n",
       "      <td>0.164091</td>\n",
       "      <td>0.262518</td>\n",
       "      <td>0.122306</td>\n",
       "      <td>0.798727</td>\n",
       "      <td>0.078967</td>\n",
       "      <td>-0.122306</td>\n",
       "      <td>5.010182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-28</th>\n",
       "      <td>5.465995</td>\n",
       "      <td>4.603197</td>\n",
       "      <td>4.970854</td>\n",
       "      <td>0.079920</td>\n",
       "      <td>10.298355</td>\n",
       "      <td>3.407179</td>\n",
       "      <td>-0.012732</td>\n",
       "      <td>-0.003930</td>\n",
       "      <td>-0.046041</td>\n",
       "      <td>-0.086817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474720</td>\n",
       "      <td>0.107600</td>\n",
       "      <td>0.583700</td>\n",
       "      <td>0.108700</td>\n",
       "      <td>0.101580</td>\n",
       "      <td>0.339672</td>\n",
       "      <td>0.579781</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>-0.339672</td>\n",
       "      <td>4.997449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-29</th>\n",
       "      <td>5.451075</td>\n",
       "      <td>4.576558</td>\n",
       "      <td>4.972933</td>\n",
       "      <td>0.099664</td>\n",
       "      <td>10.282801</td>\n",
       "      <td>3.460723</td>\n",
       "      <td>-0.050366</td>\n",
       "      <td>-0.056554</td>\n",
       "      <td>-0.075931</td>\n",
       "      <td>-0.076712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423050</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.591000</td>\n",
       "      <td>0.121900</td>\n",
       "      <td>0.445760</td>\n",
       "      <td>0.277046</td>\n",
       "      <td>0.666928</td>\n",
       "      <td>0.056026</td>\n",
       "      <td>-0.277046</td>\n",
       "      <td>4.947083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-30</th>\n",
       "      <td>5.431517</td>\n",
       "      <td>4.558223</td>\n",
       "      <td>4.974732</td>\n",
       "      <td>0.107418</td>\n",
       "      <td>10.265541</td>\n",
       "      <td>3.453790</td>\n",
       "      <td>-0.030500</td>\n",
       "      <td>-0.093598</td>\n",
       "      <td>-0.100035</td>\n",
       "      <td>-0.116721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433178</td>\n",
       "      <td>0.067000</td>\n",
       "      <td>0.500556</td>\n",
       "      <td>0.099111</td>\n",
       "      <td>0.372389</td>\n",
       "      <td>0.422049</td>\n",
       "      <td>0.491158</td>\n",
       "      <td>0.086792</td>\n",
       "      <td>-0.422049</td>\n",
       "      <td>4.916583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>688 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                MSFT     GOOGL   DEXJPUS   DEXUSUK       DJIA    VIXCLS  \\\n",
       "2019-12-11  4.977415  4.205386  4.688316  0.275812  10.236787  2.707383   \n",
       "2019-12-12  4.987516  4.208535  4.692815  0.272543  10.244665  2.634762   \n",
       "2019-12-13  4.995899  4.207333  4.693913  0.288856  10.244783  2.536075   \n",
       "2019-12-16  5.002349  4.217549  4.697202  0.287432  10.248349  2.496506   \n",
       "2019-12-17  4.996934  4.213270  4.696107  0.271248  10.249456  2.508786   \n",
       "...              ...       ...       ...       ...        ...       ...   \n",
       "2022-09-26  5.450865  4.584228  4.972933  0.067939  10.284004  3.473828   \n",
       "2022-09-27  5.446476  4.577379  4.974732  0.072600  10.279695  3.484312   \n",
       "2022-09-28  5.465995  4.603197  4.970854  0.079920  10.298355  3.407179   \n",
       "2022-09-29  5.451075  4.576558  4.972933  0.099664  10.282801  3.460723   \n",
       "2022-09-30  5.431517  4.558223  4.974732  0.107418  10.265541  3.453790   \n",
       "\n",
       "            AAPL_1DT  AAPL_3DT  AAPL_6DT  AAPL_12DT  ...  \\\n",
       "2019-12-11  0.008493  0.000221  0.042705   0.033765  ...   \n",
       "2019-12-12  0.002545  0.016866  0.036463   0.018928  ...   \n",
       "2019-12-13  0.013501  0.024540  0.035400   0.040269  ...   \n",
       "2019-12-16  0.016973  0.033020  0.033241   0.043900  ...   \n",
       "2019-12-17  0.001963  0.032438  0.049304   0.048068  ...   \n",
       "...              ...       ...       ...        ...  ...   \n",
       "2022-09-26  0.002258 -0.019377  0.000464  -0.024180  ...   \n",
       "2022-09-27  0.006545 -0.006437 -0.017764  -0.036299  ...   \n",
       "2022-09-28 -0.012732 -0.003930 -0.046041  -0.086817  ...   \n",
       "2022-09-29 -0.050366 -0.056554 -0.075931  -0.076712  ...   \n",
       "2022-09-30 -0.030500 -0.093598 -0.100035  -0.116721  ...   \n",
       "\n",
       "            sia_cleaned_Comment_tokens_joined_lemmatized_compound  \\\n",
       "2019-12-11                                           0.252900       \n",
       "2019-12-12                                           0.489700       \n",
       "2019-12-13                                           0.169000       \n",
       "2019-12-16                                           0.246837       \n",
       "2019-12-17                                           0.114757       \n",
       "...                                                       ...       \n",
       "2022-09-26                                           0.027250       \n",
       "2022-09-27                                           0.489700       \n",
       "2022-09-28                                           0.474720       \n",
       "2022-09-29                                           0.423050       \n",
       "2022-09-30                                           0.433178       \n",
       "\n",
       "            sia_cleaned_title_self_tokens_joined_lemmatized_neg  \\\n",
       "2019-12-11                                           0.035250     \n",
       "2019-12-12                                           0.028143     \n",
       "2019-12-13                                           0.056000     \n",
       "2019-12-16                                           0.053000     \n",
       "2019-12-17                                           0.094429     \n",
       "...                                                       ...     \n",
       "2022-09-26                                           0.070667     \n",
       "2022-09-27                                           0.082818     \n",
       "2022-09-28                                           0.107600     \n",
       "2022-09-29                                           0.087200     \n",
       "2022-09-30                                           0.067000     \n",
       "\n",
       "            sia_cleaned_title_self_tokens_joined_lemmatized_neu  \\\n",
       "2019-12-11                                           0.759500     \n",
       "2019-12-12                                           0.685714     \n",
       "2019-12-13                                           0.394833     \n",
       "2019-12-16                                           0.736125     \n",
       "2019-12-17                                           0.688857     \n",
       "...                                                       ...     \n",
       "2022-09-26                                           0.818000     \n",
       "2022-09-27                                           0.662273     \n",
       "2022-09-28                                           0.583700     \n",
       "2022-09-29                                           0.591000     \n",
       "2022-09-30                                           0.500556     \n",
       "\n",
       "            sia_cleaned_title_self_tokens_joined_lemmatized_pos  \\\n",
       "2019-12-11                                           0.205000     \n",
       "2019-12-12                                           0.143143     \n",
       "2019-12-13                                           0.049167     \n",
       "2019-12-16                                           0.210875     \n",
       "2019-12-17                                           0.217000     \n",
       "...                                                       ...     \n",
       "2022-09-26                                           0.111333     \n",
       "2022-09-27                                           0.164091     \n",
       "2022-09-28                                           0.108700     \n",
       "2022-09-29                                           0.121900     \n",
       "2022-09-30                                           0.099111     \n",
       "\n",
       "            sia_cleaned_title_self_tokens_joined_lemmatized_compound  \\\n",
       "2019-12-11                                           0.914275          \n",
       "2019-12-12                                           0.484814          \n",
       "2019-12-13                                          -0.120667          \n",
       "2019-12-16                                           0.646775          \n",
       "2019-12-17                                           0.640243          \n",
       "...                                                       ...          \n",
       "2022-09-26                                           0.437117          \n",
       "2022-09-27                                           0.262518          \n",
       "2022-09-28                                           0.101580          \n",
       "2022-09-29                                           0.445760          \n",
       "2022-09-30                                           0.372389          \n",
       "\n",
       "            negative_finbert_comment  neutral_finbert_comment  \\\n",
       "2019-12-11                  0.300685                 0.635741   \n",
       "2019-12-12                  0.077541                 0.784491   \n",
       "2019-12-13                  0.182847                 0.756541   \n",
       "2019-12-16                  0.264257                 0.644742   \n",
       "2019-12-17                  0.119574                 0.825515   \n",
       "...                              ...                      ...   \n",
       "2022-09-26                  0.324316                 0.595453   \n",
       "2022-09-27                  0.122306                 0.798727   \n",
       "2022-09-28                  0.339672                 0.579781   \n",
       "2022-09-29                  0.277046                 0.666928   \n",
       "2022-09-30                  0.422049                 0.491158   \n",
       "\n",
       "            positive_finbert_comment  finbert_compound  AAPL_pred  \n",
       "2019-12-11                  0.063574         -0.300685   4.183982  \n",
       "2019-12-12                  0.137968         -0.077541   4.186527  \n",
       "2019-12-13                  0.060613         -0.182847   4.200029  \n",
       "2019-12-16                  0.091001         -0.264257   4.217002  \n",
       "2019-12-17                  0.054911         -0.119574   4.218966  \n",
       "...                              ...               ...        ...  \n",
       "2022-09-26                  0.080231         -0.324316   5.003637  \n",
       "2022-09-27                  0.078967         -0.122306   5.010182  \n",
       "2022-09-28                  0.080548         -0.339672   4.997449  \n",
       "2022-09-29                  0.056026         -0.277046   4.947083  \n",
       "2022-09-30                  0.086792         -0.422049   4.916583  \n",
       "\n",
       "[688 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine X and Y into a single DataFrame for compatibility with the provided pipeline\n",
    "df = pd.concat([X, Y], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dbfc3d9-83aa-4c40-965a-86b77bb5a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(a, min_a=None, max_a=None):\n",
    "    if min_a is None:\n",
    "        min_a, max_a = np.min(a, axis=0), np.max(a, axis=0)\n",
    "    return (a - min_a) / (max_a - min_a + 0.0001), min_a, max_a\n",
    "\n",
    "def train_model(data_loader, model, loss_function, optimizer):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for X, y in data_loader:\n",
    "        output = model(X)\n",
    "        loss = loss_function(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Train loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "\n",
    "def test_model(data_loader, model, loss_function): \n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            output = model(X)\n",
    "            total_loss += loss_function(output, y).item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Test loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "\n",
    "def predict(data_loader, model):\n",
    "    \"\"\"Just like `test_loop` function but keep track of the outputs instead of the loss\n",
    "    function.\n",
    "    \"\"\"\n",
    "    output = torch.tensor([])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, _ in data_loader:\n",
    "            y_star = model(X)\n",
    "            output = torch.cat((output, y_star), 0)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "def accuracy(y, y_star):\n",
    "    return np.mean(np.abs(y - y_star) < 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee7dbd90-fd38-44d9-8716-ff67304683f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the columns for features and target\n",
    "columns = X.columns.tolist()\n",
    "target = Y.columns[0]\n",
    "dataset = df.values\n",
    "\n",
    "# Splitting the data into train and test\n",
    "size = int(len(df) * 0.8)\n",
    "df_train = dataset[:size].copy()\n",
    "df_test = dataset[size:].copy()\n",
    "\n",
    "# Reconstruct DataFrame after splitting\n",
    "df_train = pd.DataFrame(df_train, columns=columns + [target])\n",
    "df_test = pd.DataFrame(df_test, columns=columns + [target])\n",
    "\n",
    "# Normalize the data\n",
    "df_train, min_train, max_train = normalize(df_train)\n",
    "df_test, _, _ = normalize(df_test, min_train, max_train)\n",
    "\n",
    "# Track the best configuration for the stock used\n",
    "best_params = None\n",
    "best_test_rmse = float('inf')\n",
    "best_test_accuracy = float('inf')\n",
    "\n",
    "# Add sensitivity analysis dictionary\n",
    "sensitivity_analysis = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc935e0f-0050-42ca-8316-b24decd766ea",
   "metadata": {},
   "source": [
    "Case 1: Classical LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb33d6-760f-4c6e-8295-369c66067fde",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Iterate through each hyperparameter combination\n",
    "for batch_size, sequence_length, learning_rate, num_hidden_units, num_epochs in param_grid:\n",
    "    print(f\"Evaluating configuration: batch_size={batch_size}, sequence_length={sequence_length}, \"\n",
    "          f\"learning_rate={learning_rate}, hidden_units={num_hidden_units}, num_epochs={num_epochs}\")\n",
    "    \n",
    "    train_dataset = SequenceDataset(\n",
    "        df_train,\n",
    "        target=target,\n",
    "        features=columns,\n",
    "        sequence_length=sequence_length\n",
    "    )\n",
    "    test_dataset = SequenceDataset(\n",
    "        df_test,\n",
    "        target=target,\n",
    "        features=columns,\n",
    "        sequence_length=sequence_length\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    X, y = next(iter(train_loader))\n",
    "\n",
    "    print(\"Features shape:\", X.shape)\n",
    "    print(\"Target shape:\", y.shape)\n",
    "    \n",
    "    model = ShallowRegressionLSTM(\n",
    "        num_sensors=len(columns),\n",
    "        hidden_units=num_hidden_units,\n",
    "        num_layers=1\n",
    "    )\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Count number of parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Number of parameters: {num_params}\")\n",
    "\n",
    "    classical_loss_train = []\n",
    "    classical_loss_test = []\n",
    "    print(\"Untrained test\\n--------\")\n",
    "    test_loss = test_model(test_loader, model, loss_function)\n",
    "    \n",
    "    for ix_epoch in range(num_epochs):\n",
    "        print(f\"Epoch {ix_epoch}\\n---------\")\n",
    "        train_loss = train_model(train_loader, model, loss_function, optimizer=optimizer)\n",
    "        test_loss = test_model(test_loader, model, loss_function)\n",
    "        classical_loss_train.append(train_loss)\n",
    "        classical_loss_test.append(test_loss)\n",
    "        print(\"\")\n",
    "\n",
    "    train_eval_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_eval_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    ystar_col_Q = \"Model forecast\"\n",
    "    df_train[ystar_col_Q] = predict(train_eval_loader, model).numpy()\n",
    "    df_test[ystar_col_Q] = predict(test_eval_loader, model).numpy()\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(range(len(df_train)), df_train[target], label=\"Real data\")\n",
    "    plt.plot(range(len(df_train)), df_train[ystar_col_Q], label=\"LSTM train prediction\")\n",
    "    plt.ylabel('Target')\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(range(len(df_test)), df_test[target], label=\"Real data\")\n",
    "    plt.plot(range(len(df_test)), df_test[ystar_col_Q], label=\"LSTM test prediction\")\n",
    "    plt.ylabel('Target')\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    df_test.to_csv('Test_LSTM_results.csv', index=False)\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(classical_loss_train, label='Train Loss')\n",
    "    plt.plot(classical_loss_test, label='Test Loss')\n",
    "    pd.DataFrame(classical_loss_train).to_csv('LSTM_loss.csv', index=False)\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    train_rmse = math.sqrt(mean_squared_error(df_train[target], df_train[ystar_col_Q]))\n",
    "    test_rmse = math.sqrt(mean_squared_error(df_test[target], df_test[ystar_col_Q]))\n",
    "    print(f\"Train RMSE: {train_rmse}\")\n",
    "    print(f\"Test RMSE: {test_rmse}\")\n",
    "\n",
    "    train_accuracy = accuracy(df_train[target], df_train[ystar_col_Q])\n",
    "    test_accuracy = accuracy(df_test[target], df_test[ystar_col_Q])\n",
    "    print(f\"Train accuracy: {train_accuracy}\")\n",
    "    print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Track sequence length sensitivity\n",
    "    if sequence_length not in sensitivity_analysis:\n",
    "        sensitivity_analysis[sequence_length] = []\n",
    "    sensitivity_analysis[sequence_length].append((test_rmse, test_accuracy))\n",
    "\n",
    "    # Update the best parameters if necessary\n",
    "    if test_rmse < best_test_rmse or (test_rmse == best_test_rmse and test_accuracy > best_test_accuracy):\n",
    "        best_test_rmse = test_rmse\n",
    "        best_test_accuracy = test_accuracy  # Track the accuracy of the best configuration\n",
    "        best_params = {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"sequence_length\": sequence_length,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"hidden_units\": num_hidden_units,\n",
    "            \"num_epochs\": num_epochs\n",
    "        }\n",
    "    \n",
    "    print(f\"Best configuration: {best_params} with test RMSE {best_test_rmse} and test accuracy {best_test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce72d43-c345-4dd1-9873-1e83d705a554",
   "metadata": {},
   "source": [
    "Case 2: Quantum LSTM (QLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e66eb2-cfc9-4459-8675-7bff287f4a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each hyperparameter combination\n",
    "for batch_size, sequence_length, learning_rate, num_hidden_units, num_epochs in param_grid:\n",
    "    print(f\"Evaluating configuration: batch_size={batch_size}, sequence_length={sequence_length}, \"\n",
    "          f\"learning_rate={learning_rate}, hidden_units={num_hidden_units}, num_epochs={num_epochs}\")\n",
    "\n",
    "    train_dataset = SequenceDataset(\n",
    "        df_train,\n",
    "        target=target,\n",
    "        features=columns,\n",
    "        sequence_length=sequence_length\n",
    "    )\n",
    "    test_dataset = SequenceDataset(\n",
    "        df_test,\n",
    "        target=target,\n",
    "        features=columns,\n",
    "        sequence_length=sequence_length\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    X, y = next(iter(train_loader))\n",
    "    \n",
    "    print(\"Features shape:\", X.shape)\n",
    "    print(\"Target shape:\", y.shape)\n",
    "    \n",
    "    Qmodel = QShallowRegressionLSTM(\n",
    "        num_sensors=len(columns), \n",
    "        hidden_units=num_hidden_units, \n",
    "        n_qubits=7,\n",
    "        n_qlayers=1\n",
    "    )\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(Qmodel.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Count number of parameters\n",
    "    num_params = sum(p.numel() for p in Qmodel.parameters() if p.requires_grad)\n",
    "    print(f\"Number of parameters: {num_params}\")\n",
    "    \n",
    "    quantum_loss_train = []\n",
    "    quantum_loss_test = []\n",
    "    print(\"Untrained test\\n--------\")\n",
    "    \n",
    "    for ix_epoch in range(num_epochs):\n",
    "        print(f\"Epoch {ix_epoch}\\n---------\")\n",
    "        start = time.time()\n",
    "        train_loss = train_model(train_loader, Qmodel, loss_function, optimizer=optimizer)\n",
    "        test_loss = test_model(test_loader, Qmodel, loss_function)\n",
    "        end = time.time()\n",
    "        print(\"Execution time\", end - start)\n",
    "        quantum_loss_train.append(train_loss)\n",
    "        quantum_loss_test.append(test_loss)\n",
    "        print(\"\")\n",
    "    \n",
    "    train_eval_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_eval_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    ystar_col_Q = \"Model forecast\"\n",
    "    df_train[ystar_col_Q] = predict(train_eval_loader, Qmodel).numpy()\n",
    "    df_test[ystar_col_Q] = predict(test_eval_loader, Qmodel).numpy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(range(len(df_train)), df_train[target], label=\"Real data\")\n",
    "    plt.plot(range(len(df_train)), df_train[ystar_col_Q], label=\"QLSTM train prediction\")\n",
    "    plt.ylabel('Target')\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(range(len(df_test)), df_test[target], label=\"Real data\")\n",
    "    plt.plot(range(len(df_test)), df_test[ystar_col_Q], label=\"QLSTM test prediction\")\n",
    "    plt.ylabel('Target')\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    df_test.to_csv('SPY_QLSTM_test.csv', index=False)\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(quantum_loss_train, label='quantum_loss_train')\n",
    "    plt.plot(quantum_loss_test, label='quantum_loss_test')\n",
    "    pd.DataFrame(quantum_loss_train).to_csv('QLSTM_loss.csv', index=False)\n",
    "    plt.title('Train loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Calculate the RMSE for the train and test data\n",
    "    train_rmse = math.sqrt(mean_squared_error(df_train[target], df_train[ystar_col_Q]))\n",
    "    test_rmse = math.sqrt(mean_squared_error(df_test[target], df_test[ystar_col_Q]))\n",
    "    print(f\"Train RMSE: {train_rmse}\")\n",
    "    print(f\"Test RMSE: {test_rmse}\")\n",
    "    \n",
    "    # Calculate the accuracy of the model\n",
    "    train_accuracy = accuracy(df_train[target], df_train[ystar_col_Q])\n",
    "    test_accuracy = accuracy(df_test[target], df_test[ystar_col_Q])\n",
    "    print(f\"Train accuracy: {train_accuracy}\")\n",
    "    print(f\"Test accuracy: {test_accuracy}\")\n",
    "    \n",
    "    # Track sequence length sensitivity\n",
    "    if sequence_length not in sensitivity_analysis:\n",
    "        sensitivity_analysis[sequence_length] = []\n",
    "    sensitivity_analysis[sequence_length].append((test_rmse, test_accuracy))\n",
    "\n",
    "    # Update the best parameters if necessary\n",
    "    if test_rmse < best_test_rmse or (test_rmse == best_test_rmse and test_accuracy > best_test_accuracy):\n",
    "        best_test_rmse = test_rmse\n",
    "        best_test_accuracy = test_accuracy  # Track the accuracy of the best configuration\n",
    "        best_params = {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"sequence_length\": sequence_length,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"hidden_units\": num_hidden_units,\n",
    "            \"num_epochs\": num_epochs\n",
    "        }\n",
    "    \n",
    "    print(f\"Best configuration: {best_params} with test RMSE {best_test_rmse} and test accuracy {best_test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0717c4-e9d1-4ea0-bcdf-1f42a15e63e7",
   "metadata": {},
   "source": [
    "Case 3: Noisy QLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d39c311-db5a-4038-bfe3-264331f77170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each hyperparameter combination\n",
    "for batch_size, sequence_length, learning_rate, num_hidden_units, num_epochs in param_grid:\n",
    "    print(f\"Evaluating configuration: batch_size={batch_size}, sequence_length={sequence_length}, \"\n",
    "          f\"learning_rate={learning_rate}, hidden_units={num_hidden_units}, num_epochs={num_epochs}\")\n",
    "\n",
    "    train_dataset = SequenceDataset(\n",
    "        df_train,\n",
    "        target=target,\n",
    "        features=columns,\n",
    "        sequence_length=sequence_length\n",
    "    )\n",
    "    test_dataset = SequenceDataset(\n",
    "        df_test,\n",
    "        target=target,\n",
    "        features=columns,\n",
    "        sequence_length=sequence_length\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    X, y = next(iter(train_loader))\n",
    "    \n",
    "    print(\"Features shape:\", X.shape)\n",
    "    print(\"Target shape:\", y.shape)\n",
    "    \n",
    "    Qmodel = QShallowRegressionLSTM(\n",
    "        num_sensors=len(columns), \n",
    "        hidden_units=num_hidden_units, \n",
    "        n_qubits=7,\n",
    "        n_qlayers=1\n",
    "    )\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(Qmodel.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Count number of parameters\n",
    "    num_params = sum(p.numel() for p in Qmodel.parameters() if p.requires_grad)\n",
    "    print(f\"Number of parameters: {num_params}\")\n",
    "    \n",
    "    quantum_loss_train = []\n",
    "    quantum_loss_test = []\n",
    "    print(\"Untrained test\\n--------\")\n",
    "    \n",
    "    for ix_epoch in range(num_epochs):\n",
    "        print(f\"Epoch {ix_epoch}\\n---------\")\n",
    "        start = time.time()\n",
    "        train_loss = train_model(train_loader, Qmodel, loss_function, optimizer=optimizer)\n",
    "        test_loss = test_model(test_loader, Qmodel, loss_function)\n",
    "        end = time.time()\n",
    "        print(\"Execution time\", end - start)\n",
    "        quantum_loss_train.append(train_loss)\n",
    "        quantum_loss_test.append(test_loss)\n",
    "        print(\"\")\n",
    "    \n",
    "    train_eval_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_eval_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    ystar_col_Q = \"Model forecast\"\n",
    "    df_train[ystar_col_Q] = predict(train_eval_loader, Qmodel).numpy()\n",
    "    df_test[ystar_col_Q] = predict(test_eval_loader, Qmodel).numpy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(range(len(df_train)), df_train[target], label=\"Real data\")\n",
    "    plt.plot(range(len(df_train)), df_train[ystar_col_Q], label=\"QLSTM-noisy train prediction\")\n",
    "    plt.ylabel('Target')\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(range(len(df_test)), df_test[target], label=\"Real data\")\n",
    "    plt.plot(range(len(df_test)), df_test[ystar_col_Q], label=\"QLSTM-noisy test prediction\")\n",
    "    plt.ylabel('Target')\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    df_test.to_csv('SPY_QLSTM-noisy_test.csv', index=False)\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(quantum_loss_train, label='quantum_loss_train')\n",
    "    plt.plot(quantum_loss_test, label='quantum_loss_test')\n",
    "    pd.DataFrame(quantum_loss_train).to_csv('QLSTM-noisy_loss.csv', index=False)\n",
    "    plt.title('Train loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Calculate the RMSE for the train and test data\n",
    "    train_rmse = math.sqrt(mean_squared_error(df_train[target], df_train[ystar_col_Q]))\n",
    "    test_rmse = math.sqrt(mean_squared_error(df_test[target], df_test[ystar_col_Q]))\n",
    "    print(f\"Train RMSE: {train_rmse}\")\n",
    "    print(f\"Test RMSE: {test_rmse}\")\n",
    "    \n",
    "    # Calculate the accuracy of the model\n",
    "    train_accuracy = accuracy(df_train[target], df_train[ystar_col_Q])\n",
    "    test_accuracy = accuracy(df_test[target], df_test[ystar_col_Q])\n",
    "    print(f\"Train accuracy: {train_accuracy}\")\n",
    "    print(f\"Test accuracy: {test_accuracy}\")\n",
    "    \n",
    "    # Track sequence length sensitivity\n",
    "    if sequence_length not in sensitivity_analysis:\n",
    "        sensitivity_analysis[sequence_length] = []\n",
    "    sensitivity_analysis[sequence_length].append((test_rmse, test_accuracy))\n",
    "\n",
    "    # Update the best parameters if necessary\n",
    "    if test_rmse < best_test_rmse or (test_rmse == best_test_rmse and test_accuracy > best_test_accuracy):\n",
    "        best_test_rmse = test_rmse\n",
    "        best_test_accuracy = test_accuracy  # Track the accuracy of the best configuration\n",
    "        best_params = {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"sequence_length\": sequence_length,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"hidden_units\": num_hidden_units,\n",
    "            \"num_epochs\": num_epochs\n",
    "        }\n",
    "    \n",
    "    print(f\"Best configuration: {best_params} with test RMSE {best_test_rmse} and test accuracy {best_test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
