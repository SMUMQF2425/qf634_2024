{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934645c4-f8cf-4afa-9a35-e12ccb269e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\vista\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\vista\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vista\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vista\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\vista\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vista\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\vista\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\vista\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\vista\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vista\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vista\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vista\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vista\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\vista\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\vista\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\vista\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vista\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "# You may need to install libraries\n",
    "! pip install pandas\n",
    "! pip install nltk\n",
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a994078b-3a84-4ae3-a556-430c1af83f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf26056-02df-4b71-ab56-f1b3b7ca90f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "\n",
       "                                                                                                                                                       message  \n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "messages = pd.read_csv(\n",
    "    \"spam.csv\", encoding=\"latin-1\", header = 0, usecols=['v1','v2'])\n",
    "messages = messages.rename(columns={'v1':'label','v2':'message'})\n",
    "messages.head(3)\n",
    "# encoding converts bytes to character in latin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa34c4a-b4a2-4df6-88e6-d61088cae183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we define a text_preprocess method that removes punctuations, stop-words, and non-alphabets.\n",
    "\n",
    "def text_preprocess(message):\n",
    "    # Remove punctuations\n",
    "    nopunc = [char for char in message if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again\n",
    "    nopunc = \"\".join(nopunc)\n",
    "    nopunc = nopunc.lower()\n",
    "\n",
    "    # Remove any stopwords and non-alphabetic characters\n",
    "    nostop = [\n",
    "        word\n",
    "        for word in nopunc.split()\n",
    "        if word.lower() not in stopwords.words(\"english\") and word.isalpha()\n",
    "    ]\n",
    "\n",
    "    return nostop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc1b9c4e-1d13-4e15-b554-410459f6ecec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spam messages: 747\n",
      "Number of ham messages: 4825\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many spam and ham (non-spam) messages constitute our dataset.\n",
    "spam_messages = messages[messages[\"label\"] == \"spam\"][\"message\"]\n",
    "ham_messages = messages[messages[\"label\"] == \"ham\"][\"message\"]\n",
    "print(f\"Number of spam messages: {len(spam_messages)}\")\n",
    "print(f\"Number of ham messages: {len(ham_messages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6be3ab23-1a83-4efe-8b2b-72ee14667a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vista\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 spam words are:\n",
      " call      347\n",
      "free      216\n",
      "txt       150\n",
      "u         147\n",
      "ur        144\n",
      "mobile    123\n",
      "text      120\n",
      "claim     113\n",
      "stop      113\n",
      "reply     101\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Next, we check the top ten words that repeat the most in spam messages.\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Words in spam messages\n",
    "spam_words = []\n",
    "for each_message in spam_messages:\n",
    "    spam_words += text_preprocess(each_message)\n",
    "    \n",
    "print(f\"Top 10 spam words are:\\n {pd.Series(spam_words).value_counts().head(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a31f0fa7-d5fc-49b8-9c9b-81f6d4ab3cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ham words are:\n",
      " u       972\n",
      "im      449\n",
      "get     303\n",
      "ltgt    276\n",
      "ok      272\n",
      "dont    257\n",
      "go      247\n",
      "ur      240\n",
      "ill     236\n",
      "know    232\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Next, we check the top ten words that repeat the most in ham messages.\n",
    "# Words in ham messages\n",
    "ham_words = []\n",
    "for each_message in ham_messages:\n",
    "    ham_words += text_preprocess(each_message)\n",
    "    \n",
    "print(f\"Top 10 ham words are:\\n {pd.Series(ham_words).value_counts().head(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d97084f-bc02-415b-8109-38f10eb423ab",
   "metadata": {},
   "source": [
    "This information isn't needed to conduct our modeling; however, it is critical to perform exploratory data analysis to help inform our model.\r\n",
    "\r\n",
    "Here comes the crucial step: we text_preprocess our messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b2ade30-34c5-406d-8b72-92c46d81b396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>[free, entry, wkly, comp, win, fa, cup, final, tkts, may, text, fa, receive, entry, questionstd, txt, ratetcs, apply]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                 message  \n",
       "0                    [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]  \n",
       "1                                                                                         [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, wkly, comp, win, fa, cup, final, tkts, may, text, fa, receive, entry, questionstd, txt, ratetcs, apply]  \n",
       "3                                                                          [u, dun, say, early, hor, u, c, already, say]  \n",
       "4                                                                   [nah, dont, think, goes, usf, lives, around, though]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuations/stopwords from all messages\n",
    "messages[\"message\"] = messages[\"message\"].apply(text_preprocess)\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae8b56e-d31d-4289-958b-1685c87089e3",
   "metadata": {},
   "source": [
    "The output produced will be a list of tokens. A string can be understood by a model, not a list of tokens. Hence, we convert the list of tokens to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72a39a5e-e04d-44d8-83a6-acc45d334eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point crazy available bugis n great world la e buffet cine got amore wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts may text fa receive entry questionstd txt ratetcs apply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah dont think goes usf lives around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                              message  \n",
       "0                  go jurong point crazy available bugis n great world la e buffet cine got amore wat  \n",
       "1                                                                             ok lar joking wif u oni  \n",
       "2  free entry wkly comp win fa cup final tkts may text fa receive entry questionstd txt ratetcs apply  \n",
       "3                                                                 u dun say early hor u c already say  \n",
       "4                                                         nah dont think goes usf lives around though  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert messages (as lists of string tokens) to strings\n",
    "messages[\"message\"] = messages[\"message\"].transform(lambda x: \" \".join(map(str, x)))\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa95d56-8a97-4c10-a1a9-03473a568c7b",
   "metadata": {},
   "source": [
    "The CountVectorizer() class in the scikit-learn library is useful in defining the BoW approach. We first fit the vectorizer to the messages to fetch the whole vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6aeb41f-7e60-45fa-911b-3a0ccd411e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize count vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "bow_transformer = vectorizer.fit(messages[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e151faa4-7639-4219-b676-456a8167b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feature names\n",
    "features = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79553081-6675-423b-9a75-b9a400d5b83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 BOW Features: ['aa' 'aah' 'aaniye' 'aaooooright' 'aathilove' 'aathiwhere' 'ab' 'abbey'\n",
      " 'abdomen' 'abeg' 'abel' 'aberdeen' 'abi' 'ability' 'abiola' 'abj' 'able'\n",
      " 'abnormally' 'aboutas' 'abroad' 'absence' 'absolutely' 'abstract' 'abt'\n",
      " 'abta' 'aburo' 'abuse' 'abusers' 'ac' 'academic']\n",
      "Total number of vocab words: 8084\n"
     ]
    }
   ],
   "source": [
    "# Fetch the vocabulary set\n",
    "print(f\"30 BOW Features: {features[0:30]}\")\n",
    "print(f\"Total number of vocab words: {len(vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e6f8e3-12b6-410a-84c8-bb35dd40e53e",
   "metadata": {},
   "source": [
    "As can be inferred, there are about 8084 words in the text corpus we fetched.\r\n",
    "\r\n",
    "We transform the string messages to numerical vectors to simplify the model-building and training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a21c51b-ce51-4d4a-a4d9-374c77c95f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sparse matrix: (5572, 8084)\n",
      "Amount of non-zero occurrences: 44211\n"
     ]
    }
   ],
   "source": [
    "# Convert strings to vectors using BoW\n",
    "messages_bow = bow_transformer.transform(messages[\"message\"])\n",
    "\n",
    "# Print the shape of the sparse matrix and count the number of non-zero occurrences\n",
    "print(f\"Shape of sparse matrix: {messages_bow.shape}\")\n",
    "print(f\"Amount of non-zero occurrences: {messages_bow.nnz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e52dea6-1d83-4d43-bf51-ebd7ccbf1ecc",
   "metadata": {},
   "source": [
    "BoW builds a sparse matrix mapping the occurrence of every word to the corpus vocabulary. Thus, this approach leads to building a sparse matrix, or a matrix that is mostly comprised of zeros. This format allows for the conversion of the text into an interpretable encoding of linguistic information that a model can make use of."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af67fa6d-acfb-45b4-97cf-6ef0d2ae938c",
   "metadata": {},
   "source": [
    "#n the Bag of Words (BoW) section, we learned how BoW’s technique could be enhanced when combined with TF-IDF. Here, we run our BoW vectors through TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "521c2de5-4c30-4fa0-9c5b-5f80053e5858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 8084)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "\n",
    "# Transform entire BoW into tf-idf corpus\n",
    "messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "print(messages_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53450789-68d3-49d9-a543-1f101d65abd2",
   "metadata": {},
   "source": [
    "## XGBoost is a gradient boosting technique that can do both regression and classification. In this case, we will be using an XGBClassifier to classify our text as either \"ham\" or \"spam\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf6c30c-b0e2-4e7f-87d5-bc23e9632771",
   "metadata": {},
   "source": [
    "First, we convert the “spam” and “ham” labels to 0 and 1 (or vice-versa) as XGBoost accepts only numerics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65bf7803-7c8b-4495-b25d-afa145bb8dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go jurong point crazy available bugis n great world la e buffet cine got amore wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts may text fa receive entry questionstd txt ratetcs apply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah dont think goes usf lives around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      1   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                                                              message  \n",
       "0                  go jurong point crazy available bugis n great world la e buffet cine got amore wat  \n",
       "1                                                                             ok lar joking wif u oni  \n",
       "2  free entry wkly comp win fa cup final tkts may text fa receive entry questionstd txt ratetcs apply  \n",
       "3                                                                 u dun say early hor u c already say  \n",
       "4                                                         nah dont think goes usf lives around though  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert ham and spam labels to 0 and 1 (or, vice-versa)\n",
    "FactorResult = pd.factorize(messages[\"label\"])\n",
    "messages[\"label\"] = FactorResult[0]\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397ce32f-9b97-4102-8746-2c9d243b70c7",
   "metadata": {},
   "source": [
    "Next, we split the data to train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dab5841-654d-4987-a9b0-5ad613fc0224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset features size: (4457, 8084)\n",
      "train dataset label size: (4457,)\n",
      "test dataset features size: (1115, 8084)\n",
      "test dataset label size: (1115,)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset to train and test sets\n",
    "msg_train, msg_test, label_train, label_test = train_test_split(\n",
    "    messages_tfidf, messages[\"label\"], test_size=0.2\n",
    ")\n",
    "\n",
    "print(f\"train dataset features size: {msg_train.shape}\")\n",
    "print(f\"train dataset label size: {label_train.shape}\")\n",
    "\n",
    "print(f\"test dataset features size: {msg_test.shape}\")\n",
    "print(f\"test dataset label size: {label_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1b9b05-22f9-497a-b9df-a08aba85e011",
   "metadata": {},
   "source": [
    "## To train the model, we first install the XGBoost library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dab72bfb-5f81-4408-ab72-8027bf82b19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\vista\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\vista\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\vista\\anaconda3\\lib\\site-packages (from xgboost) (1.11.4)\n"
     ]
    }
   ],
   "source": [
    "# Install xgboost library\n",
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86a97ab1-2d62-4955-b10d-bb7fab41e836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We train the classifier.\n",
    "# Train an xgboost classifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Instantiate our model\n",
    "clf = XGBClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(msg_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc178e5d-6950-4400-9e61-f81eef1df955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Train dataset: 0.989\n"
     ]
    }
   ],
   "source": [
    "# Next, we make predictions on the training dataset.\n",
    "# Make predictions\n",
    "predict_train = clf.predict(msg_train)\n",
    "\n",
    "print(\n",
    "    f\"Accuracy of Train dataset: {metrics.accuracy_score(label_train, predict_train):0.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a5723-f002-4556-84cd-8956da1b9124",
   "metadata": {},
   "source": [
    "## To get an essence of how our model fared, let’s do an example prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "baebc5d0-ba15-473f-b49c-3c50c2602fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: 0\n",
      "expected: 0\n"
     ]
    }
   ],
   "source": [
    "# an example prediction\n",
    "print(\n",
    "    \"predicted:\",\n",
    "    clf.predict(\n",
    "        tfidf_transformer.transform(bow_transformer.transform([messages[\"message\"][7]]))\n",
    "    )[0],\n",
    ")\n",
    "print(\"expected:\", messages[\"label\"][7])\n",
    "### Recall Spam is \"1\", Ham is \"0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35b7c7f4-70d6-4191-8966-01a83e75e102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per request melle melle oru minnaminunginte nurungu vettam set callertune callers press copy friends callertune\n"
     ]
    }
   ],
   "source": [
    "print(messages[\"message\"][7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7518de1-999f-489b-9abc-5be61ac57871",
   "metadata": {},
   "source": [
    "And yes, it worked!\r\n",
    "\r\n",
    "Finally, we find the overall accuracy of the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f1f5807-6558-4549-a7c9-0c25ee09be59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.966\n"
     ]
    }
   ],
   "source": [
    "# print the overall accuracy of the model\n",
    "label_predictions = clf.predict(msg_test)\n",
    "print(f\"Accuracy of the model: {metrics.accuracy_score(label_test, label_predictions):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cc6923c-7630-4f3b-9fdc-32a1a05c0b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: 1\n"
     ]
    }
   ],
   "source": [
    "# here is an out-of-sample generalized prediction\n",
    "print(\n",
    "    \"predicted:\",\n",
    "    clf.predict(\n",
    "        tfidf_transformer.transform(bow_transformer.transform([\"Hullo, \\\n",
    "        claim your free luck draw by sending some money\"])))[0],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "520e844c-428f-4b60-8f14-214935819395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7849)\t0.23822187576133733\n",
      "  (0, 7624)\t0.19696340342726038\n",
      "  (0, 5218)\t0.2701917502132373\n",
      "  (0, 3740)\t0.2918335915067389\n",
      "  (0, 3591)\t0.3454453072521403\n",
      "  (0, 2824)\t0.19227005405586492\n",
      "  (0, 2787)\t0.16228382867264401\n",
      "  (0, 2733)\t0.15705625796538664\n",
      "  (0, 1483)\t0.26752361109476797\n",
      "  (0, 1208)\t0.2918335915067389\n",
      "  (0, 897)\t0.2918335915067389\n",
      "  (0, 895)\t0.3297648855969494\n",
      "  (0, 464)\t0.2626832171423389\n",
      "  (0, 233)\t0.3454453072521403\n",
      "  (1, 7750)\t0.4313385731102942\n",
      "  (1, 4838)\t0.5462557824449175\n",
      "  (1, 4806)\t0.2741803362458879\n",
      "  (1, 3776)\t0.4080505634471753\n",
      "  (1, 3559)\t0.5233273701797004\n",
      "  (2, 7803)\t0.21180477976633422\n",
      "  (2, 7764)\t0.1655268599567031\n",
      "  (2, 7295)\t0.14030009452821\n",
      "  (2, 7092)\t0.2454421736459655\n",
      "  (2, 6937)\t0.1372922225787766\n",
      "  (2, 5602)\t0.18586214999858788\n",
      "  (2, 5548)\t0.2610826711059533\n",
      "  (2, 5497)\t0.2610826711059533\n",
      "  (2, 4194)\t0.17816738588670292\n",
      "  (2, 2533)\t0.1288894945470838\n",
      "  (2, 2380)\t0.2045670103380392\n",
      "  (2, 2256)\t0.5221653422119066\n",
      "  (2, 2124)\t0.40015658822537653\n",
      "  (2, 1536)\t0.22744527722632202\n",
      "  (2, 1319)\t0.21863700584342968\n",
      "  (2, 319)\t0.1886059018292755\n"
     ]
    }
   ],
   "source": [
    "print(messages_tfidf[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fffc31fb-6f1d-4cfc-aace-f9507d5ece74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 233)\t1\n",
      "  (0, 464)\t1\n",
      "  (0, 895)\t1\n",
      "  (0, 897)\t1\n",
      "  (0, 1208)\t1\n",
      "  (0, 1483)\t1\n",
      "  (0, 2733)\t1\n",
      "  (0, 2787)\t1\n",
      "  (0, 2824)\t1\n",
      "  (0, 3591)\t1\n",
      "  (0, 3740)\t1\n",
      "  (0, 5218)\t1\n",
      "  (0, 7624)\t1\n",
      "  (0, 7849)\t1\n",
      "  (1, 3559)\t1\n",
      "  (1, 3776)\t1\n",
      "  (1, 4806)\t1\n",
      "  (1, 4838)\t1\n",
      "  (1, 7750)\t1\n",
      "  (2, 319)\t1\n",
      "  (2, 1319)\t1\n",
      "  (2, 1536)\t1\n",
      "  (2, 2124)\t2\n",
      "  (2, 2256)\t2\n",
      "  (2, 2380)\t1\n",
      "  (2, 2533)\t1\n",
      "  (2, 4194)\t1\n",
      "  (2, 5497)\t1\n",
      "  (2, 5548)\t1\n",
      "  (2, 5602)\t1\n",
      "  (2, 6937)\t1\n",
      "  (2, 7092)\t1\n",
      "  (2, 7295)\t1\n",
      "  (2, 7764)\t1\n",
      "  (2, 7803)\t1\n"
     ]
    }
   ],
   "source": [
    "print(messages_bow[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b37dfd-d550-4767-bdb8-c7f4b5617dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
