{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18cf589a-8c79-43c9-a28a-fd979a9692e1",
   "metadata": {},
   "source": [
    "Ref source: Topic Modeling and Latent Dirichlet Allocation (LDA) in Python by Susan Li published in Towards Data Science 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f70c271-3a40-4036-a668-66855fbfa7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('abcnews-date-text.csv',  on_bad_lines='skip');\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "### Note: argument 'error_bad_lines=False' has been deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65bcfd70-b439-4a33-92b2-98627bc2b0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1244184\n",
      "                                       headline_text  index\n",
      "0  aba decides against community broadcasting lic...      0\n",
      "1     act fire witnesses must be aware of defamation      1\n",
      "2     a g calls for infrastructure protection summit      2\n",
      "3           air nz staff in aust strike for pay rise      3\n",
      "4      air nz strike to affect australian travellers      4\n"
     ]
    }
   ],
   "source": [
    "### Take a look at data/abc news headlines\n",
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba347a43-edf3-4a08-be7f-6ded1cc0b6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f29858a-6829-4382-a621-59e1abafc2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1244179</th>\n",
       "      <td>20211231</td>\n",
       "      <td>two aged care residents die as state records 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244180</th>\n",
       "      <td>20211231</td>\n",
       "      <td>victoria records 5;919 new cases and seven deaths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244181</th>\n",
       "      <td>20211231</td>\n",
       "      <td>wa delays adopting new close contact definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244182</th>\n",
       "      <td>20211231</td>\n",
       "      <td>western ringtail possums found badly dehydrate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244183</th>\n",
       "      <td>20211231</td>\n",
       "      <td>what makes you a close covid contact here are ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         publish_date                                      headline_text\n",
       "1244179      20211231  two aged care residents die as state records 2...\n",
       "1244180      20211231  victoria records 5;919 new cases and seven deaths\n",
       "1244181      20211231    wa delays adopting new close contact definition\n",
       "1244182      20211231  western ringtail possums found badly dehydrate...\n",
       "1244183      20211231  what makes you a close covid contact here are ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc64a34-b5d8-4888-9374-5fb897c28d2b",
   "metadata": {},
   "source": [
    "### Text data pre-processing involving the following steps\n",
    "Tokenization: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "Words that have fewer than 3 characters are removed.\n",
    "All stopwords are removed.\n",
    "Words are lemmatized — words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "Words are stemmed — words are reduced to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dd04e3e-ce50-4e00-b27c-77e378279624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyStemmer in c:\\users\\vista\\anaconda3\\lib\\site-packages (2.2.0.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install PyStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b86ccda7-87e6-4efb-8ec5-0b578dce0620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vista\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Load gensim and nltk libraries\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ae65870-ca25-4c3a-aee2-af54ed3bfe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write a function to perform lemmatize and stem preprocessing steps on the data set.\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ec42e19-dc3b-461a-8e9d-dbc5436d3803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['rail', 'tunnel', 'plan', 'may', 'be', 'back', 'on', 'state', 'agenda']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['rail', 'tunnel', 'plan', 'state', 'agenda']\n"
     ]
    }
   ],
   "source": [
    "### Select a document to preview after preprocessing.\n",
    "doc_sample = documents[documents['index'] == 8972].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f69a39a2-c27f-438f-b011-5d1b92925399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        headline_text  index\n",
      "8970         raa predicts drop in country fuel prices   8970\n",
      "8971  rail brochure scrapped over townsville omission   8971\n",
      "8972     rail tunnel plan may be back on state agenda   8972\n",
      "8973            rain leads to canning river fish kill   8973\n",
      "8974  raising teenagers could be a poverty trap acoss   8974\n"
     ]
    }
   ],
   "source": [
    "### This is how the above looks like in the original headline text\n",
    "print(documents[8970:8975])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88df828a-8821-409f-b3e2-4c64b9128a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               [decid, commun, broadcast, licenc]\n",
       "1                               [wit, awar, defam]\n",
       "2           [call, infrastructur, protect, summit]\n",
       "3                      [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travel]\n",
       "5               [ambiti, olsson, win, tripl, jump]\n",
       "6           [antic, delight, record, break, barca]\n",
       "7    [aussi, qualifi, stosur, wast, memphi, match]\n",
       "8            [aust, address, secur, council, iraq]\n",
       "9                         [australia, lock, timet]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Preprocess the headline text, saving the results as ‘processed_docs’\n",
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65d11eeb-c841-4241-81af-44a3cf1cee91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1244184"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1aa459-a0e3-428c-95ef-f04a1f240c84",
   "metadata": {},
   "source": [
    "# Bag of Words on the Data set\r\n",
    "Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81be64de-797c-4c7b-9c1c-13749fee1187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 commun\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n",
      "11 aust\n",
      "12 rise\n",
      "13 staff\n",
      "14 strike\n",
      "15 affect\n",
      "16 australian\n",
      "17 travel\n",
      "18 ambiti\n",
      "19 jump\n",
      "20 olsson\n",
      "21 tripl\n",
      "22 win\n",
      "23 antic\n",
      "24 barca\n",
      "25 break\n",
      "26 delight\n",
      "27 record\n",
      "28 aussi\n",
      "29 match\n",
      "30 memphi\n",
      "31 qualifi\n",
      "32 stosur\n",
      "33 wast\n",
      "34 address\n",
      "35 council\n",
      "36 iraq\n",
      "37 secur\n",
      "38 australia\n",
      "39 lock\n",
      "40 timet\n",
      "41 contribut\n",
      "42 million\n",
      "43 birthday\n",
      "44 celebr\n",
      "45 robson\n",
      "46 ahead\n",
      "47 bathhous\n",
      "48 plan\n",
      "49 championship\n",
      "50 cycl\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c0ff1ca-78f0-4136-a23a-bef3c43dc73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usaustralia\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70293"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dictionary[70292])\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669dd448-b73d-4585-a719-5ec416dc1c8a",
   "metadata": {},
   "source": [
    "Gensim filter_extremes\n",
    "\n",
    "Filter out tokens that appear in less than 15 documents (absolute number) i.e. rare words\n",
    "or more than 0.5 documents (fraction of total corpus size), i.e. overly common words.\n",
    "after the above two steps, keep only the first 100000 most frequent tokens.\n",
    "### dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bcfb9ef-9d4e-4138-861d-c7285f718e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=1, no_above=0.99, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e43551f4-1561-4b20-9b3c-6d4afd65d6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70293"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d346e194-fb69-41a0-bd09-1510b3d56b93",
   "metadata": {},
   "source": [
    "Gensim doc2bow\r\n",
    "\r\n",
    "For each document we create a dictionary reporting how many\r\n",
    "words and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff312a89-08a9-4860-9553-fb2939ef8f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(48, 1), (60, 1), (923, 1), (1468, 1), (2827, 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[8972]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422d18ad-0705-47f4-a0d9-3add47d3ec99",
   "metadata": {},
   "source": [
    "# Preview Bag Of Words for our sample preprocessed document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7418d38f-a1af-4d13-8492-13d9afd4ae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 48 (\"plan\") appears 1 time.\n",
      "Word 60 (\"state\") appears 1 time.\n",
      "Word 923 (\"tunnel\") appears 1 time.\n",
      "Word 1468 (\"rail\") appears 1 time.\n",
      "Word 2827 (\"agenda\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_8972 = bow_corpus[8972]\n",
    "for i in range(len(bow_doc_8972)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_8972[i][0], \n",
    "                                               dictionary[bow_doc_8972[i][0]], \n",
    "bow_doc_8972[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d41615d5-1a62-40a8-a937-c8f0157d36f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1244184"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "887675a2-0b15-4b98-bda2-313e0acbc7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1)]\n",
      "[(4, 1), (5, 1), (6, 1)]\n",
      "[(7, 1), (8, 1), (9, 1), (10, 1)]\n",
      "[(11, 1), (12, 1), (13, 1), (14, 1)]\n",
      "[(14, 1), (15, 1), (16, 1), (17, 1)]\n",
      "[(18, 1), (19, 1), (20, 1), (21, 1), (22, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(bow_corpus[0])\n",
    "print(bow_corpus[1])\n",
    "print(bow_corpus[2])\n",
    "print(bow_corpus[3])\n",
    "print(bow_corpus[4])\n",
    "print(bow_corpus[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da522077-b318-4b9e-96c1-ab6008bf2267",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85aa58c3-5ac1-4bcb-af37-efe122312f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5854395661274623),\n",
      " (1, 0.383252758688686),\n",
      " (2, 0.50230806644029),\n",
      " (3, 0.5080004367704987)]\n",
      "[(4, 0.6001950088242418), (5, 0.6146521710780535), (6, 0.5118287408611435)]\n",
      "[(7, 0.3823222189653971),\n",
      " (8, 0.5648602809024891),\n",
      " (9, 0.47289714459609433),\n",
      " (10, 0.5577910671362503)]\n",
      "[(11, 0.5358712461682118),\n",
      " (12, 0.43512898451094045),\n",
      " (13, 0.53611420657169),\n",
      " (14, 0.485887159616935)]\n",
      "[(14, 0.47235494692269364),\n",
      " (15, 0.5715156253616932),\n",
      " (16, 0.38223210042906264),\n",
      " (17, 0.5514973395100641)]\n",
      "[(18, 0.4989690427684149),\n",
      " (19, 0.35526178458502394),\n",
      " (20, 0.6399442306983866),\n",
      " (21, 0.38583734587630736),\n",
      " (22, 0.2577205519457323)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "\n",
    "count = 0\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    count += 1\n",
    "    if count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c05581d-49a5-479c-be9e-338b50be0569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1244184"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc297d98-2333-429d-8794-1c84be343bc4",
   "metadata": {},
   "source": [
    "# Running LDA using Bag of Words\n",
    "Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9b72012-eea0-44d1-8769-cd5de76ee5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcac7dd7-1214-4a8d-a2de-6d8a249ad526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.036*\"trump\" + 0.032*\"sydney\" + 0.024*\"melbourn\" + 0.023*\"charg\" + 0.023*\"court\" + 0.023*\"china\" + 0.020*\"donald\" + 0.019*\"murder\" + 0.017*\"face\" + 0.016*\"woman\"\n",
      "Topic: 1 \n",
      "Words: 0.056*\"covid\" + 0.044*\"australian\" + 0.031*\"coronaviru\" + 0.022*\"vaccin\" + 0.018*\"open\" + 0.016*\"case\" + 0.015*\"australia\" + 0.015*\"world\" + 0.010*\"win\" + 0.009*\"break\"\n",
      "Topic: 2 \n",
      "Words: 0.039*\"victoria\" + 0.021*\"warn\" + 0.020*\"adelaid\" + 0.016*\"final\" + 0.013*\"travel\" + 0.013*\"andrew\" + 0.012*\"street\" + 0.012*\"coronaviru\" + 0.011*\"time\" + 0.011*\"hotel\"\n",
      "Topic: 3 \n",
      "Words: 0.028*\"australia\" + 0.028*\"south\" + 0.018*\"north\" + 0.017*\"victorian\" + 0.017*\"test\" + 0.016*\"miss\" + 0.015*\"coronaviru\" + 0.014*\"west\" + 0.013*\"lose\" + 0.011*\"search\"\n",
      "Topic: 4 \n",
      "Words: 0.030*\"year\" + 0.021*\"women\" + 0.021*\"record\" + 0.018*\"border\" + 0.015*\"speak\" + 0.014*\"life\" + 0.013*\"australia\" + 0.012*\"sentenc\" + 0.012*\"farm\" + 0.012*\"abus\"\n",
      "Topic: 5 \n",
      "Words: 0.029*\"kill\" + 0.016*\"dead\" + 0.014*\"protest\" + 0.014*\"presid\" + 0.013*\"biden\" + 0.012*\"attack\" + 0.010*\"australia\" + 0.010*\"northern\" + 0.009*\"right\" + 0.009*\"celebr\"\n",
      "Topic: 6 \n",
      "Words: 0.030*\"elect\" + 0.015*\"lockdown\" + 0.013*\"coast\" + 0.012*\"gold\" + 0.012*\"labor\" + 0.011*\"power\" + 0.011*\"tasmanian\" + 0.011*\"million\" + 0.011*\"countri\" + 0.009*\"polit\"\n",
      "Topic: 7 \n",
      "Words: 0.062*\"polic\" + 0.033*\"death\" + 0.026*\"live\" + 0.023*\"famili\" + 0.022*\"crash\" + 0.020*\"die\" + 0.019*\"canberra\" + 0.016*\"investig\" + 0.013*\"offic\" + 0.012*\"interview\"\n",
      "Topic: 8 \n",
      "Words: 0.026*\"govern\" + 0.019*\"chang\" + 0.018*\"news\" + 0.017*\"nation\" + 0.015*\"bushfir\" + 0.015*\"plan\" + 0.014*\"tasmania\" + 0.013*\"school\" + 0.012*\"commun\" + 0.011*\"island\"\n",
      "Topic: 9 \n",
      "Words: 0.047*\"queensland\" + 0.021*\"market\" + 0.019*\"coronaviru\" + 0.017*\"rise\" + 0.016*\"morrison\" + 0.016*\"scott\" + 0.014*\"high\" + 0.013*\"leav\" + 0.013*\"price\" + 0.013*\"quarantin\"\n"
     ]
    }
   ],
   "source": [
    "### For each topic, we will explore the words occuring in that topic and its relative weight.\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0818db9-0f10-46f6-84c1-8b00d7cb555c",
   "metadata": {},
   "source": [
    "Can you distinguish different topics using the words in each topic and their corresponding weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec28a299-f12b-46c8-aa78-5a5851e52458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.036*\"trump\" + 0.032*\"sydney\" + 0.024*\"melbourn\" + 0.023*\"charg\" + 0.023*\"court\" + 0.023*\"china\" + 0.020*\"donald\" + 0.019*\"murder\" + 0.017*\"face\" + 0.016*\"woman\" + 0.015*\"peopl\" + 0.015*\"restrict\" + 0.015*\"brisban\" + 0.013*\"trial\" + 0.012*\"case\" + 0.012*\"accus\" + 0.010*\"care\" + 0.010*\"flood\" + 0.009*\"tell\" + 0.009*\"alleg\" + 0.009*\"age\" + 0.009*\"drug\" + 0.009*\"guilti\" + 0.008*\"hear\" + 0.008*\"storm\" + 0.008*\"home\" + 0.007*\"arrest\" + 0.006*\"intern\" + 0.006*\"bring\" + 0.006*\"hobart\"')\n",
      "(1, '0.056*\"covid\" + 0.044*\"australian\" + 0.031*\"coronaviru\" + 0.022*\"vaccin\" + 0.018*\"open\" + 0.016*\"case\" + 0.015*\"australia\" + 0.015*\"world\" + 0.010*\"win\" + 0.009*\"break\" + 0.008*\"game\" + 0.007*\"updat\" + 0.007*\"health\" + 0.007*\"test\" + 0.007*\"race\" + 0.007*\"mental\" + 0.006*\"river\" + 0.006*\"stori\" + 0.006*\"aborigin\" + 0.006*\"free\" + 0.006*\"sport\" + 0.006*\"unit\" + 0.006*\"peter\" + 0.006*\"minist\" + 0.005*\"action\" + 0.005*\"play\" + 0.005*\"hill\" + 0.005*\"olymp\" + 0.005*\"team\" + 0.005*\"histori\"')\n",
      "(2, '0.039*\"victoria\" + 0.021*\"warn\" + 0.020*\"adelaid\" + 0.016*\"final\" + 0.013*\"travel\" + 0.013*\"andrew\" + 0.012*\"street\" + 0.012*\"coronaviru\" + 0.011*\"time\" + 0.011*\"hotel\" + 0.010*\"risk\" + 0.010*\"season\" + 0.010*\"australia\" + 0.010*\"chines\" + 0.009*\"john\" + 0.009*\"wall\" + 0.009*\"premier\" + 0.009*\"media\" + 0.009*\"weather\" + 0.008*\"drought\" + 0.008*\"futur\" + 0.008*\"issu\" + 0.007*\"brief\" + 0.007*\"friday\" + 0.007*\"fire\" + 0.007*\"shark\" + 0.007*\"make\" + 0.006*\"footag\" + 0.006*\"grand\" + 0.006*\"struggl\"')\n",
      "(3, '0.028*\"australia\" + 0.028*\"south\" + 0.018*\"north\" + 0.017*\"victorian\" + 0.017*\"test\" + 0.016*\"miss\" + 0.015*\"coronaviru\" + 0.014*\"west\" + 0.013*\"lose\" + 0.011*\"search\" + 0.010*\"continu\" + 0.010*\"drum\" + 0.010*\"young\" + 0.009*\"say\" + 0.009*\"student\" + 0.009*\"leagu\" + 0.009*\"doctor\" + 0.008*\"financ\" + 0.008*\"extend\" + 0.008*\"drive\" + 0.008*\"grow\" + 0.008*\"flight\" + 0.008*\"east\" + 0.007*\"coast\" + 0.007*\"korea\" + 0.007*\"outbreak\" + 0.007*\"monday\" + 0.007*\"hong\" + 0.007*\"kong\" + 0.006*\"coal\"')\n",
      "(4, '0.030*\"year\" + 0.021*\"women\" + 0.021*\"record\" + 0.018*\"border\" + 0.015*\"speak\" + 0.014*\"life\" + 0.013*\"australia\" + 0.012*\"sentenc\" + 0.012*\"farm\" + 0.012*\"abus\" + 0.011*\"darwin\" + 0.011*\"emerg\" + 0.010*\"india\" + 0.010*\"liber\" + 0.009*\"jail\" + 0.009*\"violenc\" + 0.009*\"take\" + 0.009*\"close\" + 0.009*\"black\" + 0.009*\"explain\" + 0.008*\"western\" + 0.008*\"head\" + 0.008*\"daniel\" + 0.008*\"track\" + 0.008*\"child\" + 0.007*\"facebook\" + 0.007*\"tuesday\" + 0.006*\"domest\" + 0.006*\"thursday\" + 0.006*\"univers\"')\n",
      "(5, '0.029*\"kill\" + 0.016*\"dead\" + 0.014*\"protest\" + 0.014*\"presid\" + 0.013*\"biden\" + 0.012*\"attack\" + 0.010*\"australia\" + 0.010*\"northern\" + 0.009*\"right\" + 0.009*\"celebr\" + 0.008*\"latest\" + 0.008*\"sale\" + 0.008*\"shoot\" + 0.008*\"say\" + 0.008*\"battl\" + 0.008*\"georg\" + 0.008*\"know\" + 0.007*\"near\" + 0.007*\"princ\" + 0.007*\"youth\" + 0.007*\"economi\" + 0.006*\"human\" + 0.006*\"catch\" + 0.006*\"arriv\" + 0.006*\"scientist\" + 0.006*\"anim\" + 0.006*\"harri\" + 0.006*\"bomb\" + 0.006*\"militari\" + 0.006*\"foreign\"')\n",
      "(6, '0.030*\"elect\" + 0.015*\"lockdown\" + 0.013*\"coast\" + 0.012*\"gold\" + 0.012*\"labor\" + 0.011*\"power\" + 0.011*\"tasmanian\" + 0.011*\"million\" + 0.011*\"countri\" + 0.009*\"polit\" + 0.009*\"say\" + 0.008*\"vote\" + 0.008*\"parti\" + 0.008*\"parliament\" + 0.007*\"work\" + 0.007*\"zealand\" + 0.007*\"council\" + 0.007*\"food\" + 0.006*\"hous\" + 0.006*\"cost\" + 0.006*\"delay\" + 0.006*\"senat\" + 0.006*\"compani\" + 0.006*\"week\" + 0.006*\"green\" + 0.006*\"energi\" + 0.006*\"johnson\" + 0.006*\"turn\" + 0.005*\"campaign\" + 0.005*\"festiv\"')\n",
      "(7, '0.062*\"polic\" + 0.033*\"death\" + 0.026*\"live\" + 0.023*\"famili\" + 0.022*\"crash\" + 0.020*\"die\" + 0.019*\"canberra\" + 0.016*\"investig\" + 0.013*\"offic\" + 0.012*\"interview\" + 0.011*\"victim\" + 0.011*\"driver\" + 0.010*\"train\" + 0.010*\"rescu\" + 0.009*\"video\" + 0.009*\"sexual\" + 0.008*\"fatal\" + 0.008*\"bodi\" + 0.007*\"road\" + 0.007*\"girl\" + 0.007*\"alan\" + 0.007*\"target\" + 0.006*\"babi\" + 0.006*\"inquest\" + 0.006*\"steal\" + 0.006*\"shoot\" + 0.006*\"wednesday\" + 0.006*\"grant\" + 0.006*\"rape\" + 0.006*\"safeti\"')\n",
      "(8, '0.026*\"govern\" + 0.019*\"chang\" + 0.018*\"news\" + 0.017*\"nation\" + 0.015*\"bushfir\" + 0.015*\"plan\" + 0.014*\"tasmania\" + 0.013*\"school\" + 0.012*\"commun\" + 0.011*\"island\" + 0.011*\"indigen\" + 0.011*\"health\" + 0.011*\"feder\" + 0.011*\"water\" + 0.010*\"region\" + 0.010*\"fund\" + 0.010*\"royal\" + 0.009*\"say\" + 0.009*\"busi\" + 0.009*\"local\" + 0.008*\"climat\" + 0.008*\"need\" + 0.008*\"servic\" + 0.008*\"commiss\" + 0.008*\"announc\" + 0.008*\"park\" + 0.008*\"pandem\" + 0.008*\"budget\" + 0.008*\"rural\" + 0.007*\"reveal\"')\n",
      "(9, '0.047*\"queensland\" + 0.021*\"market\" + 0.019*\"coronaviru\" + 0.017*\"rise\" + 0.016*\"morrison\" + 0.016*\"scott\" + 0.014*\"high\" + 0.013*\"leav\" + 0.013*\"price\" + 0.013*\"quarantin\" + 0.012*\"amid\" + 0.012*\"fall\" + 0.011*\"bank\" + 0.010*\"prison\" + 0.009*\"mark\" + 0.009*\"share\" + 0.009*\"record\" + 0.009*\"michael\" + 0.009*\"year\" + 0.008*\"hit\" + 0.008*\"australia\" + 0.008*\"david\" + 0.008*\"number\" + 0.008*\"releas\" + 0.008*\"star\" + 0.007*\"forc\" + 0.007*\"job\" + 0.006*\"strike\" + 0.006*\"cut\" + 0.006*\"rat\"')\n"
     ]
    }
   ],
   "source": [
    "# Print the topics\n",
    "topics = lda_model.print_topics(num_words=30)  # You can specify the number of words per topic\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd597e-33b2-43e7-965f-2ac1da5ecad2",
   "metadata": {},
   "source": [
    "# Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a4def72-c952-4feb-af74-22f9dc2bdaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.011*\"interview\" + 0.010*\"scott\" + 0.008*\"andrew\" + 0.008*\"weather\" + 0.006*\"queensland\" + 0.006*\"search\" + 0.006*\"david\" + 0.006*\"dollar\" + 0.006*\"australia\" + 0.006*\"juli\"\n",
      "Topic: 1 Word: 0.015*\"drum\" + 0.010*\"stori\" + 0.010*\"friday\" + 0.009*\"tuesday\" + 0.009*\"sport\" + 0.008*\"michael\" + 0.008*\"korea\" + 0.007*\"flight\" + 0.006*\"daniel\" + 0.005*\"wild\"\n",
      "Topic: 2 Word: 0.017*\"covid\" + 0.015*\"coronaviru\" + 0.011*\"restrict\" + 0.008*\"case\" + 0.007*\"updat\" + 0.007*\"record\" + 0.006*\"govern\" + 0.006*\"victoria\" + 0.005*\"australia\" + 0.005*\"august\"\n",
      "Topic: 3 Word: 0.011*\"australia\" + 0.009*\"govern\" + 0.007*\"age\" + 0.006*\"world\" + 0.006*\"australian\" + 0.006*\"peter\" + 0.006*\"cricket\" + 0.006*\"win\" + 0.005*\"alan\" + 0.005*\"celebr\"\n",
      "Topic: 4 Word: 0.031*\"trump\" + 0.018*\"donald\" + 0.012*\"lockdown\" + 0.012*\"coronaviru\" + 0.007*\"turnbul\" + 0.007*\"covid\" + 0.006*\"disabl\" + 0.006*\"footag\" + 0.005*\"economi\" + 0.005*\"onlin\"\n",
      "Topic: 5 Word: 0.016*\"charg\" + 0.016*\"murder\" + 0.015*\"polic\" + 0.012*\"court\" + 0.010*\"alleg\" + 0.009*\"woman\" + 0.009*\"child\" + 0.009*\"assault\" + 0.009*\"sentenc\" + 0.008*\"shoot\"\n",
      "Topic: 6 Word: 0.013*\"crash\" + 0.010*\"morrison\" + 0.010*\"coast\" + 0.009*\"kill\" + 0.009*\"royal\" + 0.009*\"bushfir\" + 0.008*\"monday\" + 0.007*\"commiss\" + 0.007*\"dead\" + 0.006*\"christma\"\n",
      "Topic: 7 Word: 0.008*\"presid\" + 0.007*\"violenc\" + 0.007*\"wednesday\" + 0.007*\"video\" + 0.006*\"elect\" + 0.006*\"biden\" + 0.006*\"liber\" + 0.006*\"say\" + 0.005*\"domest\" + 0.005*\"refuge\"\n",
      "Topic: 8 Word: 0.026*\"news\" + 0.014*\"vaccin\" + 0.014*\"rural\" + 0.011*\"covid\" + 0.010*\"coronaviru\" + 0.009*\"street\" + 0.009*\"wall\" + 0.008*\"quarantin\" + 0.008*\"nation\" + 0.007*\"market\"\n",
      "Topic: 9 Word: 0.011*\"countri\" + 0.010*\"health\" + 0.008*\"hour\" + 0.008*\"chang\" + 0.007*\"climat\" + 0.007*\"budget\" + 0.006*\"fund\" + 0.006*\"feder\" + 0.006*\"coronaviru\" + 0.006*\"elect\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b838a8-c61f-421d-8744-728d70f3abad",
   "metadata": {},
   "source": [
    "Again, can you distinguish different topics using the words in each topic and their corresponding weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0dee46-3c65-4ea1-91b1-295e02c05a78",
   "metadata": {},
   "source": [
    "# Performance evaluation by classifying sample document using LDA Bag of Words model\n",
    "We will check where (to which topic) our test document would be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0e39a76-1604-4e7d-ad65-c1350cf470dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9dd3465-f278-43f0-b595-5b2fcc5eb718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.5960099101066589\t \n",
      "Topic: 0.026*\"govern\" + 0.019*\"chang\" + 0.018*\"news\" + 0.017*\"nation\" + 0.015*\"bushfir\" + 0.015*\"plan\" + 0.014*\"tasmania\" + 0.013*\"school\" + 0.012*\"commun\" + 0.011*\"island\"\n",
      "\n",
      "Score: 0.30394983291625977\t \n",
      "Topic: 0.030*\"elect\" + 0.015*\"lockdown\" + 0.013*\"coast\" + 0.012*\"gold\" + 0.012*\"labor\" + 0.011*\"power\" + 0.011*\"tasmanian\" + 0.011*\"million\" + 0.011*\"countri\" + 0.009*\"polit\"\n",
      "\n",
      "Score: 0.012505189515650272\t \n",
      "Topic: 0.056*\"covid\" + 0.044*\"australian\" + 0.031*\"coronaviru\" + 0.022*\"vaccin\" + 0.018*\"open\" + 0.016*\"case\" + 0.015*\"australia\" + 0.015*\"world\" + 0.010*\"win\" + 0.009*\"break\"\n",
      "\n",
      "Score: 0.01250517088919878\t \n",
      "Topic: 0.028*\"australia\" + 0.028*\"south\" + 0.018*\"north\" + 0.017*\"victorian\" + 0.017*\"test\" + 0.016*\"miss\" + 0.015*\"coronaviru\" + 0.014*\"west\" + 0.013*\"lose\" + 0.011*\"search\"\n",
      "\n",
      "Score: 0.012505164369940758\t \n",
      "Topic: 0.030*\"year\" + 0.021*\"women\" + 0.021*\"record\" + 0.018*\"border\" + 0.015*\"speak\" + 0.014*\"life\" + 0.013*\"australia\" + 0.012*\"sentenc\" + 0.012*\"farm\" + 0.012*\"abus\"\n",
      "\n",
      "Score: 0.012504946440458298\t \n",
      "Topic: 0.036*\"trump\" + 0.032*\"sydney\" + 0.024*\"melbourn\" + 0.023*\"charg\" + 0.023*\"court\" + 0.023*\"china\" + 0.020*\"donald\" + 0.019*\"murder\" + 0.017*\"face\" + 0.016*\"woman\"\n",
      "\n",
      "Score: 0.012504944577813148\t \n",
      "Topic: 0.039*\"victoria\" + 0.021*\"warn\" + 0.020*\"adelaid\" + 0.016*\"final\" + 0.013*\"travel\" + 0.013*\"andrew\" + 0.012*\"street\" + 0.012*\"coronaviru\" + 0.011*\"time\" + 0.011*\"hotel\"\n",
      "\n",
      "Score: 0.012504944577813148\t \n",
      "Topic: 0.029*\"kill\" + 0.016*\"dead\" + 0.014*\"protest\" + 0.014*\"presid\" + 0.013*\"biden\" + 0.012*\"attack\" + 0.010*\"australia\" + 0.010*\"northern\" + 0.009*\"right\" + 0.009*\"celebr\"\n",
      "\n",
      "Score: 0.012504944577813148\t \n",
      "Topic: 0.062*\"polic\" + 0.033*\"death\" + 0.026*\"live\" + 0.023*\"famili\" + 0.022*\"crash\" + 0.020*\"die\" + 0.019*\"canberra\" + 0.016*\"investig\" + 0.013*\"offic\" + 0.012*\"interview\"\n",
      "\n",
      "Score: 0.012504944577813148\t \n",
      "Topic: 0.047*\"queensland\" + 0.021*\"market\" + 0.019*\"coronaviru\" + 0.017*\"rise\" + 0.016*\"morrison\" + 0.016*\"scott\" + 0.014*\"high\" + 0.013*\"leav\" + 0.013*\"price\" + 0.013*\"quarantin\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94269e93-b393-4725-a8cc-d56def558a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Our test document has the highest probability to be part of the topic that our model assigned, which is the accurate classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd9c0c-f059-4bba-9963-9c1dad07d799",
   "metadata": {},
   "source": [
    "# Performance evaluation by classifying sample document using LDA TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de1940e3-5737-406d-8474-886478fe2d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.6002472043037415\t \n",
      "Topic: 0.011*\"countri\" + 0.010*\"health\" + 0.008*\"hour\" + 0.008*\"chang\" + 0.007*\"climat\" + 0.007*\"budget\" + 0.006*\"fund\" + 0.006*\"feder\" + 0.006*\"coronaviru\" + 0.006*\"elect\"\n",
      "\n",
      "Score: 0.1645704060792923\t \n",
      "Topic: 0.031*\"trump\" + 0.018*\"donald\" + 0.012*\"lockdown\" + 0.012*\"coronaviru\" + 0.007*\"turnbul\" + 0.007*\"covid\" + 0.006*\"disabl\" + 0.006*\"footag\" + 0.005*\"economi\" + 0.005*\"onlin\"\n",
      "\n",
      "Score: 0.14761237800121307\t \n",
      "Topic: 0.016*\"charg\" + 0.016*\"murder\" + 0.015*\"polic\" + 0.012*\"court\" + 0.010*\"alleg\" + 0.009*\"woman\" + 0.009*\"child\" + 0.009*\"assault\" + 0.009*\"sentenc\" + 0.008*\"shoot\"\n",
      "\n",
      "Score: 0.012511433102190495\t \n",
      "Topic: 0.017*\"covid\" + 0.015*\"coronaviru\" + 0.011*\"restrict\" + 0.008*\"case\" + 0.007*\"updat\" + 0.007*\"record\" + 0.006*\"govern\" + 0.006*\"victoria\" + 0.005*\"australia\" + 0.005*\"august\"\n",
      "\n",
      "Score: 0.0125108752399683\t \n",
      "Topic: 0.008*\"presid\" + 0.007*\"violenc\" + 0.007*\"wednesday\" + 0.007*\"video\" + 0.006*\"elect\" + 0.006*\"biden\" + 0.006*\"liber\" + 0.006*\"say\" + 0.005*\"domest\" + 0.005*\"refuge\"\n",
      "\n",
      "Score: 0.012509901076555252\t \n",
      "Topic: 0.026*\"news\" + 0.014*\"vaccin\" + 0.014*\"rural\" + 0.011*\"covid\" + 0.010*\"coronaviru\" + 0.009*\"street\" + 0.009*\"wall\" + 0.008*\"quarantin\" + 0.008*\"nation\" + 0.007*\"market\"\n",
      "\n",
      "Score: 0.012509835883975029\t \n",
      "Topic: 0.015*\"drum\" + 0.010*\"stori\" + 0.010*\"friday\" + 0.009*\"tuesday\" + 0.009*\"sport\" + 0.008*\"michael\" + 0.008*\"korea\" + 0.007*\"flight\" + 0.006*\"daniel\" + 0.005*\"wild\"\n",
      "\n",
      "Score: 0.012509535066783428\t \n",
      "Topic: 0.011*\"australia\" + 0.009*\"govern\" + 0.007*\"age\" + 0.006*\"world\" + 0.006*\"australian\" + 0.006*\"peter\" + 0.006*\"cricket\" + 0.006*\"win\" + 0.005*\"alan\" + 0.005*\"celebr\"\n",
      "\n",
      "Score: 0.012509300373494625\t \n",
      "Topic: 0.011*\"interview\" + 0.010*\"scott\" + 0.008*\"andrew\" + 0.008*\"weather\" + 0.006*\"queensland\" + 0.006*\"search\" + 0.006*\"david\" + 0.006*\"dollar\" + 0.006*\"australia\" + 0.006*\"juli\"\n",
      "\n",
      "Score: 0.012509078718721867\t \n",
      "Topic: 0.013*\"crash\" + 0.010*\"morrison\" + 0.010*\"coast\" + 0.009*\"kill\" + 0.009*\"royal\" + 0.009*\"bushfir\" + 0.008*\"monday\" + 0.007*\"commiss\" + 0.007*\"dead\" + 0.006*\"christma\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2984b5c2-4669-4690-b9c3-abaf46357128",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Our test document has the highest probability to be part of the topic that our model assigned, which is the accurate classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a152a96-d320-4eda-ab06-d47f8756bdae",
   "metadata": {},
   "source": [
    "# Testing model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5d9b357-400b-4e9e-adcb-c716080a5273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.4200029671192169\t Topic: 0.036*\"trump\" + 0.032*\"sydney\" + 0.024*\"melbourn\" + 0.023*\"charg\" + 0.023*\"court\"\n",
      "Score: 0.2199912816286087\t Topic: 0.056*\"covid\" + 0.044*\"australian\" + 0.031*\"coronaviru\" + 0.022*\"vaccin\" + 0.018*\"open\"\n",
      "Score: 0.2199835628271103\t Topic: 0.030*\"year\" + 0.021*\"women\" + 0.021*\"record\" + 0.018*\"border\" + 0.015*\"speak\"\n",
      "Score: 0.020003177225589752\t Topic: 0.039*\"victoria\" + 0.021*\"warn\" + 0.020*\"adelaid\" + 0.016*\"final\" + 0.013*\"travel\"\n",
      "Score: 0.020003177225589752\t Topic: 0.028*\"australia\" + 0.028*\"south\" + 0.018*\"north\" + 0.017*\"victorian\" + 0.017*\"test\"\n",
      "Score: 0.020003177225589752\t Topic: 0.029*\"kill\" + 0.016*\"dead\" + 0.014*\"protest\" + 0.014*\"presid\" + 0.013*\"biden\"\n",
      "Score: 0.020003177225589752\t Topic: 0.030*\"elect\" + 0.015*\"lockdown\" + 0.013*\"coast\" + 0.012*\"gold\" + 0.012*\"labor\"\n",
      "Score: 0.020003177225589752\t Topic: 0.062*\"polic\" + 0.033*\"death\" + 0.026*\"live\" + 0.023*\"famili\" + 0.022*\"crash\"\n",
      "Score: 0.020003177225589752\t Topic: 0.026*\"govern\" + 0.019*\"chang\" + 0.018*\"news\" + 0.017*\"nation\" + 0.015*\"bushfir\"\n",
      "Score: 0.020003177225589752\t Topic: 0.047*\"queensland\" + 0.021*\"market\" + 0.019*\"coronaviru\" + 0.017*\"rise\" + 0.016*\"morrison\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'China has some of the best technologies on earth'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93828d93-d3ee-4088-9073-55a1f4dca7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d25b0d-3509-4ca8-9abd-557f010624ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
